{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11834448,"sourceType":"datasetVersion","datasetId":7434963}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **$$Assignment-3$$**","metadata":{}},{"cell_type":"markdown","source":"# Question-1","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, input_vocab_size, output_vocab_size, emb_dim=128, hidden_size=256, \n                 rnn_type='LSTM', num_layers=1, device='cpu'):\n        super(Seq2Seq, self).__init__()\n        self.device = device\n        self.emb_dim = emb_dim\n        self.hidden_size = hidden_size\n        self.rnn_type = rnn_type.upper()\n\n        self.embedding = nn.Embedding(input_vocab_size, emb_dim)\n        self.target_embedding = nn.Embedding(output_vocab_size, emb_dim)\n\n        rnn_cls = {\n            'RNN': nn.RNN,\n            'LSTM': nn.LSTM,\n            'GRU': nn.GRU\n        }[self.rnn_type]\n\n        self.encoder = rnn_cls(input_size=emb_dim, hidden_size=hidden_size, \n                               num_layers=num_layers, batch_first=True)\n\n        self.decoder = rnn_cls(input_size=emb_dim, hidden_size=hidden_size, \n                               num_layers=num_layers, batch_first=True)\n\n        self.output_layer = nn.Linear(hidden_size, output_vocab_size)\n\n    def forward(self, source, target):\n        \n        embedded_src = self.embedding(source)\n        encoder_outputs, hidden = self.encoder(embedded_src)\n\n        embedded_tgt = self.target_embedding(target)\n        decoder_outputs, _ = self.decoder(embedded_tgt, hidden)\n        output = self.output_layer(decoder_outputs)\n\n        return output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Question-2","metadata":{}},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"fb4c8007ed0d1fb692b2279b11bb69081f2c698d\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:12:51.834389Z","iopub.execute_input":"2025-05-20T12:12:51.834669Z","iopub.status.idle":"2025-05-20T12:13:02.105457Z","shell.execute_reply.started":"2025-05-20T12:12:51.834642Z","shell.execute_reply":"2025-05-20T12:13:02.104845Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mma23c014\u001b[0m (\u001b[33mma23c014-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"### Import libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport pandas as pd\nimport wandb\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T08:14:15.585610Z","iopub.execute_input":"2025-05-20T08:14:15.586051Z","iopub.status.idle":"2025-05-20T08:14:23.146785Z","shell.execute_reply.started":"2025-05-20T08:14:15.586026Z","shell.execute_reply":"2025-05-20T08:14:23.145908Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Dataset utilities\nclass TransliterationDataset(Dataset):\n    def __init__(self, pairs, input_vocab, output_vocab):\n        self.pairs = pairs\n        self.input_vocab = input_vocab\n        self.output_vocab = output_vocab\n        self.sos = output_vocab['<sos>']\n        self.eos = output_vocab['<eos>']\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        source, target = self.pairs[idx]\n        input_ids = [self.input_vocab[c] for c in source]\n        target_ids = [self.sos] + [self.output_vocab[c] for c in target] + [self.eos]\n        return torch.tensor(input_ids), torch.tensor(target_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T08:14:24.842321Z","iopub.execute_input":"2025-05-20T08:14:24.843044Z","iopub.status.idle":"2025-05-20T08:14:24.847991Z","shell.execute_reply.started":"2025-05-20T08:14:24.843020Z","shell.execute_reply":"2025-05-20T08:14:24.847345Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def build_vocab(pairs):\n    input_chars = set()\n    output_chars = set()\n    for source, target in pairs:\n        input_chars.update(source)\n        output_chars.update(target)\n    input_vocab = {c: i + 1 for i, c in enumerate(sorted(input_chars))}\n    input_vocab['<pad>'] = 0\n    output_vocab = {c: i + 3 for i, c in enumerate(sorted(output_chars))}\n    output_vocab.update({'<pad>': 0, '<sos>': 1, '<eos>': 2})\n    return input_vocab, output_vocab\n\ndef load_pairs(path):\n    df = pd.read_csv(path, sep=\"\\t\", header=None, names=[\"target\", \"source\", \"count\"], dtype=str)\n    df.dropna(subset=[\"source\", \"target\"], inplace=True)\n    return list(zip(df[\"source\"], df[\"target\"]))\n\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    input_lens = [len(seq) for seq in inputs]\n    target_lens = [len(seq) for seq in targets]\n    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n    return inputs_padded, targets_padded, input_lens, target_lens\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=0)\n        rnn_class = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n        self.rnn = rnn_class(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n\n    def forward(self, x, lengths):\n        x = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n        outputs, hidden = self.rnn(packed)\n        return hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, output_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(output_size, embed_size, padding_idx=0)\n        rnn_class = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n        self.rnn = rnn_class(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input_token, hidden):\n        x = self.embedding(input_token.unsqueeze(1))\n        output, hidden = self.rnn(x, hidden)\n        output = self.fc(output.squeeze(1))\n        return output, hidden\n\n    def beam_search(self, hidden, max_len, sos_idx, eos_idx, beam_size=3):\n        device = next(self.parameters()).device\n        sequences = [[torch.tensor([sos_idx], device=device), hidden, 0.0]]\n        completed = []\n\n        for _ in range(max_len):\n            new_sequences = []\n            for seq, h, score in sequences:\n                input_token = seq[-1].unsqueeze(0)\n                output, new_hidden = self.forward(input_token, h)\n                probs = torch.log_softmax(output, dim=-1).squeeze(0)\n                topk_probs, topk_indices = probs.topk(beam_size)\n                for i in range(beam_size):\n                    next_token = topk_indices[i].item()\n                    new_score = score + topk_probs[i].item()\n                    new_seq = torch.cat([seq, torch.tensor([next_token], device=device)])\n                    new_sequences.append([new_seq, new_hidden, new_score])\n            sequences = sorted(new_sequences, key=lambda x: x[2], reverse=True)[:beam_size]\n            completed.extend([seq for seq in sequences if seq[0][-1].item() == eos_idx])\n            sequences = [seq for seq in sequences if seq[0][-1].item() != eos_idx]\n            if not sequences:\n                break\n        completed = sorted(completed, key=lambda x: x[2], reverse=True)\n        return completed[0][0] if completed else sequences[0][0]\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, src, src_lens, tgt=None, teacher_forcing_ratio=0.5):\n        batch_size = src.size(0)\n        device = src.device\n        hidden = self.encoder(src, src_lens)\n        if tgt is not None:\n            tgt_len = tgt.size(1)\n            outputs = torch.zeros(batch_size, tgt_len, self.decoder.fc.out_features, device=device)\n            input_token = tgt[:, 0]\n            for t in range(1, tgt_len):\n                output, hidden = self.decoder(input_token, hidden)\n                outputs[:, t] = output\n                teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n                input_token = tgt[:, t] if teacher_force else output.argmax(1)\n            return outputs\n        else:\n            return [self.decoder.beam_search(hidden, max_len=20, sos_idx=1, eos_idx=2) for _ in range(batch_size)]\n\ndef accuracy(preds, targets, pad_idx=0):\n    pred_tokens = preds.argmax(dim=-1)\n    correct = ((pred_tokens == targets) & (targets != pad_idx)).sum().item()\n    total = (targets != pad_idx).sum().item()\n    return correct / total if total > 0 else 0.0\n\ndef train(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss, total_acc = 0, 0\n    for src, tgt, src_lens, tgt_lens in tqdm(loader, desc=\"Training\", leave=False):\n        src, tgt = src.to(device), tgt.to(device)\n        optimizer.zero_grad()\n        output = model(src, src_lens, tgt)\n        loss = criterion(output[:, 1:].reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n        acc = accuracy(output[:, 1:], tgt[:, 1:])\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        total_acc += acc\n    return total_loss / len(loader), total_acc / len(loader)\n\n@torch.no_grad()\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    total_loss, total_acc = 0, 0\n    for src, tgt, src_lens, tgt_lens in tqdm(loader, desc=\"Evaluating\", leave=False):\n        src, tgt = src.to(device), tgt.to(device)\n        output = model(src, src_lens, tgt, teacher_forcing_ratio=0.0)\n        loss = criterion(output[:, 1:].reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n        acc = accuracy(output[:, 1:], tgt[:, 1:])\n        total_loss += loss.item()\n        total_acc += acc\n    return total_loss / len(loader), total_acc / len(loader)\n\ndef main():\n    import wandb\n    # Run name will be assigned after wandb.init with config\n    def generate_run_name(config):\n        return f\"cell:{config.cell_type}_embed:{config.embed_size}_hid:{config.hidden_size}_layers:{config.num_layers}_beam:{config.beam_size}\"\n\n    # First initialize W&B run with placeholder name\n    wandb.init(project=\"dakshina-transliteration\", config=wandb.config)\n    config = wandb.config\n\n    # Then update the run name\n    wandb.run.name = generate_run_name(config)\n    wandb.run.save() \n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    train_pairs = load_pairs(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n    dev_pairs = load_pairs(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\")\n\n    input_vocab, output_vocab = build_vocab(train_pairs)\n    train_dataset = TransliterationDataset(train_pairs, input_vocab, output_vocab)\n    dev_dataset = TransliterationDataset(dev_pairs, input_vocab, output_vocab)\n\n    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n\n    encoder = Encoder(len(input_vocab), config.embed_size, config.hidden_size, config.num_layers, config.cell_type, config.dropout)\n    decoder = Decoder(len(output_vocab), config.embed_size, config.hidden_size, config.num_layers, config.cell_type, config.dropout)\n    model = Seq2Seq(encoder, decoder).to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    for epoch in range(10):\n        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n        val_loss, val_acc = evaluate(model, dev_loader, criterion, device)\n        wandb.log({\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"train_accuracy\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_accuracy\": val_acc\n        })\n\n\nif __name__ == \"__main__\":\n    sweep_config = {\n        \"method\": \"bayes\",\n        \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n        \"parameters\": {\n            \"embed_size\": {\"values\": [32, 64, 128]},\n            \"hidden_size\": {\"values\": [16,32, 64, 128]},\n            \"num_layers\": {\"values\": [1,2,3]},\n            \"cell_type\": {\"values\": [\"RNN\", \"GRU\", \"LSTM\"]},\n            \"dropout\": {\"values\": [0.1,0.2, 0.3]},\n            \"lr\": {\"min\": 0.0001, \"max\": 0.01},\n            \"batch_size\": {\"values\": [16,32, 64]},\n            \"beam_size\": {\"values\": [1, 3, 5]}  \n        }\n    }\n\n    sweep_id = wandb.sweep(sweep_config, project=\"dakshina-transliteration\")\n    wandb.agent(sweep_id, function=main, count=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T08:14:27.423782Z","iopub.execute_input":"2025-05-20T08:14:27.424513Z","execution_failed":"2025-05-20T12:01:55.718Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: vntbxx0z\nSweep URL: https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: luwmjfms with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.006613015850178118\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_081434-luwmjfms</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/luwmjfms' target=\"_blank\">cosmic-sweep-1</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/luwmjfms' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/luwmjfms</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n                                                              \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▄▆▆▇▇▆█▇</td></tr><tr><td>val_loss</td><td>█▅▅▃▃▃▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.73723</td></tr><tr><td>train_loss</td><td>0.86571</td></tr><tr><td>val_accuracy</td><td>0.64548</td></tr><tr><td>val_loss</td><td>1.20942</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell:LSTM_embed:64_hid:64_layers:2_beam:3</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/luwmjfms' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/luwmjfms</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_081434-luwmjfms/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0gyikyrq with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0062017648359144265\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_082514-0gyikyrq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/0gyikyrq' target=\"_blank\">lemon-sweep-2</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/0gyikyrq' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/0gyikyrq</a>"},"metadata":{}},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▇▇▇█▇█</td></tr><tr><td>val_loss</td><td>█▅▄▃▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.57346</td></tr><tr><td>train_loss</td><td>1.41401</td></tr><tr><td>val_accuracy</td><td>0.53685</td></tr><tr><td>val_loss</td><td>1.58914</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell:GRU_embed:64_hid:16_layers:2_beam:3</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/0gyikyrq' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/0gyikyrq</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_082514-0gyikyrq/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i5zv17xg with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0017595862340974528\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_083052-i5zv17xg</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/i5zv17xg' target=\"_blank\">iconic-sweep-3</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/i5zv17xg' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/i5zv17xg</a>"},"metadata":{}},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▄▃▂▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.84159</td></tr><tr><td>train_loss</td><td>0.53765</td></tr><tr><td>val_accuracy</td><td>0.67427</td></tr><tr><td>val_loss</td><td>1.18121</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell:LSTM_embed:128_hid:128_layers:1_beam:5</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/i5zv17xg' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/i5zv17xg</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_083052-i5zv17xg/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: do0ff2yj with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.002905964003211017\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_083338-do0ff2yj</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/do0ff2yj' target=\"_blank\">dashing-sweep-4</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/do0ff2yj' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/do0ff2yj</a>"},"metadata":{}},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▇▇█▇██</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.65783</td></tr><tr><td>train_loss</td><td>1.10021</td></tr><tr><td>val_accuracy</td><td>0.61429</td></tr><tr><td>val_loss</td><td>1.26838</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell:GRU_embed:32_hid:32_layers:2_beam:5</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/do0ff2yj' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/do0ff2yj</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_083338-do0ff2yj/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ii7z8zrn with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.008363730112610004\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_084334-ii7z8zrn</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/ii7z8zrn' target=\"_blank\">clean-sweep-5</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/ii7z8zrn' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/ii7z8zrn</a>"},"metadata":{}},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▆▆▇▇█████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆▇▇▇▇█▇</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.71291</td></tr><tr><td>train_loss</td><td>0.94749</td></tr><tr><td>val_accuracy</td><td>0.62658</td></tr><tr><td>val_loss</td><td>1.2639</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell:LSTM_embed:64_hid:64_layers:2_beam:3</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/ii7z8zrn' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/ii7z8zrn</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_084334-ii7z8zrn/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7doqm96b with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.006177069147855139\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_085412-7doqm96b</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/7doqm96b' target=\"_blank\">desert-sweep-6</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/7doqm96b' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/7doqm96b</a>"},"metadata":{}},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇██▇█▇█</td></tr><tr><td>val_loss</td><td>█▂▃▂▁▁▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.80125</td></tr><tr><td>train_loss</td><td>0.66363</td></tr><tr><td>val_accuracy</td><td>0.64223</td></tr><tr><td>val_loss</td><td>1.30744</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell:LSTM_embed:128_hid:128_layers:1_beam:5</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/7doqm96b' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/7doqm96b</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_085412-7doqm96b/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k95p6vca with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0007224041581262549\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_085709-k95p6vca</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/k95p6vca' target=\"_blank\">iconic-sweep-7</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/k95p6vca' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/k95p6vca</a>"},"metadata":{}},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.82725</td></tr><tr><td>train_loss</td><td>0.58199</td></tr><tr><td>val_accuracy</td><td>0.67512</td></tr><tr><td>val_loss</td><td>1.16456</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell:LSTM_embed:128_hid:128_layers:1_beam:5</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/k95p6vca' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/k95p6vca</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_085709-k95p6vca/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7ohumtoj with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.00013515486056911045\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_090156-7ohumtoj</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/7ohumtoj' target=\"_blank\">young-sweep-8</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/7ohumtoj' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/7ohumtoj</a>"},"metadata":{}},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▃▅▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▅▆▇▇▇██</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.6817</td></tr><tr><td>train_loss</td><td>1.02885</td></tr><tr><td>val_accuracy</td><td>0.60524</td></tr><tr><td>val_loss</td><td>1.28459</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell:LSTM_embed:128_hid:128_layers:2_beam:3</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/7ohumtoj' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/7ohumtoj</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_090156-7ohumtoj/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gvsgq94t with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0018526487947841857\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_090528-gvsgq94t</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/gvsgq94t' target=\"_blank\">fearless-sweep-9</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/gvsgq94t' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/gvsgq94t</a>"},"metadata":{}},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇▇▇████</td></tr><tr><td>val_loss</td><td>█▄▂▁▁▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.85848</td></tr><tr><td>train_loss</td><td>0.47855</td></tr><tr><td>val_accuracy</td><td>0.67931</td></tr><tr><td>val_loss</td><td>1.19754</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell:LSTM_embed:64_hid:128_layers:1_beam:3</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/gvsgq94t' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/gvsgq94t</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_090528-gvsgq94t/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5exdjly0 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.003024107884273066\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_091021-5exdjly0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/5exdjly0' target=\"_blank\">fluent-sweep-10</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/5exdjly0' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/5exdjly0</a>"},"metadata":{}},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▆▇▇████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇▇█████</td></tr><tr><td>val_loss</td><td>█▃▃▂▁▁▂▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.82871</td></tr><tr><td>train_loss</td><td>0.57507</td></tr><tr><td>val_accuracy</td><td>0.65638</td></tr><tr><td>val_loss</td><td>1.27698</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell:LSTM_embed:128_hid:128_layers:1_beam:5</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/5exdjly0' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/5exdjly0</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_091021-5exdjly0/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5nqdg3ve with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001882357489943128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_091514-5nqdg3ve</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/5nqdg3ve' target=\"_blank\">smart-sweep-11</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/5nqdg3ve' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/5nqdg3ve</a>"},"metadata":{}},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆▇▇████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.83835</td></tr><tr><td>train_loss</td><td>0.54706</td></tr><tr><td>val_accuracy</td><td>0.66922</td></tr><tr><td>val_loss</td><td>1.2018</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell:LSTM_embed:128_hid:128_layers:1_beam:3</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/5nqdg3ve' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/5nqdg3ve</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_091514-5nqdg3ve/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p7cv3fxs with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0022865282010076627\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_091806-p7cv3fxs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/p7cv3fxs' target=\"_blank\">crimson-sweep-12</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/p7cv3fxs' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/p7cv3fxs</a>"},"metadata":{}},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▆▇▇▇██▇</td></tr><tr><td>val_loss</td><td>█▄▂▃▃▂▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.81978</td></tr><tr><td>train_loss</td><td>0.60606</td></tr><tr><td>val_accuracy</td><td>0.65439</td></tr><tr><td>val_loss</td><td>1.24177</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell:GRU_embed:128_hid:128_layers:1_beam:5</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/p7cv3fxs' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/p7cv3fxs</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_091806-p7cv3fxs/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bumtnwsu with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0009053580454511916\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_092052-bumtnwsu</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/bumtnwsu' target=\"_blank\">crimson-sweep-13</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/bumtnwsu' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/bumtnwsu</a>"},"metadata":{}},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▄▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆▇▇████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.81936</td></tr><tr><td>train_loss</td><td>0.60462</td></tr><tr><td>val_accuracy</td><td>0.66911</td></tr><tr><td>val_loss</td><td>1.18925</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell:LSTM_embed:64_hid:128_layers:1_beam:5</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/bumtnwsu' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/bumtnwsu</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_092052-bumtnwsu/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9nfnprcr with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0013424154209032198\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_092343-9nfnprcr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/9nfnprcr' target=\"_blank\">dry-sweep-14</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/9nfnprcr' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/9nfnprcr</a>"},"metadata":{}},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.8436</td></tr><tr><td>train_loss</td><td>0.52818</td></tr><tr><td>val_accuracy</td><td>0.67391</td></tr><tr><td>val_loss</td><td>1.19112</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell:LSTM_embed:128_hid:128_layers:1_beam:5</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/9nfnprcr' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/9nfnprcr</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_092343-9nfnprcr/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vi2s4l2m with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0006421526587101498\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_092835-vi2s4l2m</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/vi2s4l2m' target=\"_blank\">hearty-sweep-15</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/vi2s4l2m' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/vi2s4l2m</a>"},"metadata":{}},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▄▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▇▇████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.80567</td></tr><tr><td>train_loss</td><td>0.64963</td></tr><tr><td>val_accuracy</td><td>0.66533</td></tr><tr><td>val_loss</td><td>1.16873</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell:LSTM_embed:128_hid:128_layers:1_beam:3</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/vi2s4l2m' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/vi2s4l2m</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_092835-vi2s4l2m/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2bm6ucl0 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001602978854417375\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_093127-2bm6ucl0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/2bm6ucl0' target=\"_blank\">devout-sweep-16</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/2bm6ucl0' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/2bm6ucl0</a>"},"metadata":{}},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇▇▇████</td></tr><tr><td>val_loss</td><td>█▄▂▁▁▁▁▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.85874</td></tr><tr><td>train_loss</td><td>0.48014</td></tr><tr><td>val_accuracy</td><td>0.6801</td></tr><tr><td>val_loss</td><td>1.21099</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell:LSTM_embed:64_hid:128_layers:1_beam:3</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/2bm6ucl0' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/2bm6ucl0</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_093127-2bm6ucl0/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bypcfej8 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001159992077775628\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_093620-bypcfej8</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/bypcfej8' target=\"_blank\">stilted-sweep-17</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/bypcfej8' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/bypcfej8</a>"},"metadata":{}},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆▇▇█▇▇█</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.84503</td></tr><tr><td>train_loss</td><td>0.52187</td></tr><tr><td>val_accuracy</td><td>0.68769</td></tr><tr><td>val_loss</td><td>1.14176</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell:LSTM_embed:128_hid:128_layers:1_beam:5</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/bypcfej8' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/bypcfej8</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_093620-bypcfej8/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: v4vadsnv with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0017154123438262545\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_094502-v4vadsnv</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/v4vadsnv' target=\"_blank\">generous-sweep-18</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/v4vadsnv' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/v4vadsnv</a>"},"metadata":{}},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇▇█████</td></tr><tr><td>val_loss</td><td>█▃▂▁▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.84435</td></tr><tr><td>train_loss</td><td>0.52439</td></tr><tr><td>val_accuracy</td><td>0.67475</td></tr><tr><td>val_loss</td><td>1.20852</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell:LSTM_embed:128_hid:128_layers:1_beam:5</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/v4vadsnv' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/v4vadsnv</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_094502-v4vadsnv/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dess47a1 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.002031253345529322\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_094955-dess47a1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/dess47a1' target=\"_blank\">cerulean-sweep-19</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/vntbxx0z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/dess47a1' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/dess47a1</a>"},"metadata":{}},{"name":"stderr","text":"Training:  93%|█████████▎| 2569/2763 [00:44<00:03, 58.17it/s] ","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Ques-4\n### Best Model on Test Data","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport pandas as pd\nimport csv\n\n# ---------------- Dataset & Utils ----------------\nclass TransliterationDataset(Dataset):\n    def __init__(self, pairs, input_vocab, output_vocab):\n        self.pairs = pairs\n        self.input_vocab = input_vocab\n        self.output_vocab = output_vocab\n        self.sos = output_vocab['<sos>']\n        self.eos = output_vocab['<eos>']\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        source, target = self.pairs[idx]\n        input_ids = [self.input_vocab[c] for c in source]\n        target_ids = [self.sos] + [self.output_vocab[c] for c in target] + [self.eos]\n        return torch.tensor(input_ids), torch.tensor(target_ids)\n\ndef load_pairs(path):\n    df = pd.read_csv(path, sep='\\t', header=None, names=['target', 'source', 'count'], dtype=str)\n    df.dropna(subset=[\"source\", \"target\"], inplace=True)\n    return list(zip(df['source'], df['target']))\n\ndef build_vocab(pairs):\n    input_chars = set()\n    output_chars = set()\n    for src, tgt in pairs:\n        input_chars.update(src)\n        output_chars.update(tgt)\n    input_vocab = {c: i+1 for i, c in enumerate(sorted(input_chars))}\n    input_vocab['<pad>'] = 0\n    output_vocab = {c: i+3 for i, c in enumerate(sorted(output_chars))}\n    output_vocab.update({'<pad>': 0, '<sos>': 1, '<eos>': 2})\n    return input_vocab, output_vocab\n\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    input_lens = [len(x) for x in inputs]\n    target_lens = [len(x) for x in targets]\n    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n    return inputs_padded, targets_padded, input_lens, target_lens\n\n# ---------------- Models ----------------\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=0)\n        rnn_cls = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n        self.rnn = rnn_cls(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n\n    def forward(self, x, lengths):\n        embedded = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n        outputs, hidden = self.rnn(packed)\n        return hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, output_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(output_size, embed_size, padding_idx=0)\n        rnn_cls = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n        self.rnn = rnn_cls(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, token, hidden):\n        x = self.embedding(token.unsqueeze(1))\n        output, hidden = self.rnn(x, hidden)\n        output = self.fc(output.squeeze(1))\n        return output, hidden\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, src, src_lens, tgt=None, teacher_forcing_ratio=0.5):\n        batch_size = src.size(0)\n        hidden = self.encoder(src, src_lens)\n        tgt_len = tgt.size(1)\n        outputs = torch.zeros(batch_size, tgt_len, self.decoder.fc.out_features).to(src.device)\n        input_token = tgt[:, 0]\n        for t in range(1, tgt_len):\n            output, hidden = self.decoder(input_token, hidden)\n            outputs[:, t] = output\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            input_token = tgt[:, t] if teacher_force else output.argmax(1)\n        return outputs\n\n# ---------------- Train + Eval ----------------\ndef train_model(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    for src, tgt, src_lens, _ in dataloader:\n        src, tgt = src.to(device), tgt.to(device)\n        optimizer.zero_grad()\n        output = model(src, src_lens, tgt)\n        loss = criterion(output[:, 1:].reshape(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n\ndef evaluate_and_save(model, dataloader, input_vocab, output_vocab, device, csv_path=None):\n    model.eval()\n    inv_input_vocab = {v: k for k, v in input_vocab.items()}\n    inv_output_vocab = {v: k for k, v in output_vocab.items()}\n    correct = 0\n    total = 0\n    results = []\n\n    with torch.no_grad():\n        for src, tgt, src_lens, _ in dataloader:\n            src = src.to(device)\n            hidden = model.encoder(src, src_lens)\n            input_token = torch.tensor([output_vocab['<sos>']] * src.size(0)).to(device)\n            decoded = []\n            for _ in range(20):\n                output, hidden = model.decoder(input_token, hidden)\n                input_token = output.argmax(1)\n                decoded.append(input_token)\n            decoded = torch.stack(decoded, dim=1)\n\n            for i in range(src.size(0)):\n                pred = ''.join([inv_output_vocab[t.item()] for t in decoded[i] if t.item() not in [output_vocab['<eos>'], 0]])\n                truth = ''.join([inv_output_vocab[t.item()] for t in tgt[i][1:-1]])\n                inp = ''.join([inv_input_vocab[t.item()] for t in src[i] if t.item() != 0])\n                results.append((inp, pred, truth))\n                if pred == truth:\n                    correct += 1\n                total += 1\n\n    acc = correct / total * 100\n    print(f\"\\n Test Accuracy: {acc:.2f}%\")\n    for inp, pred, truth in results[:10]:\n        print(f\"{inp:<15} | Pred: {pred:<20} | Truth: {truth}\")\n\n    if csv_path is not None:\n        with open(csv_path, mode='w', newline='', encoding='utf-8') as f:\n            writer = csv.writer(f)\n            writer.writerow(['Input', 'Prediction', 'GroundTruth'])\n            writer.writerows(results)\n        print(f\"\\n Predictions saved to: {csv_path}\")\n\n    return acc, results\n\n\n# ---------------- Run ----------------\nif __name__ == \"__main__\":\n    config = {\n        \"embed_size\": 128,\n        \"hidden_size\": 128,\n        \"num_layers\": 2,\n        \"cell_type\": \"LSTM\",\n        \"dropout\": 0.2,\n        \"batch_size\": 64,\n        \"lr\": 0.0013206,\n        \"epochs\": 10,\n    }\n\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    train_pairs = load_pairs(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n    test_pairs = load_pairs(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\")\n    input_vocab, output_vocab = build_vocab(train_pairs)\n    train_dataset = TransliterationDataset(train_pairs, input_vocab, output_vocab)\n    test_dataset = TransliterationDataset(test_pairs, input_vocab, output_vocab)\n\n    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=collate_fn)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n    encoder = Encoder(len(input_vocab), config[\"embed_size\"], config[\"hidden_size\"],\n                      config[\"num_layers\"], config[\"cell_type\"], config[\"dropout\"])\n    decoder = Decoder(len(output_vocab), config[\"embed_size\"], config[\"hidden_size\"],\n                      config[\"num_layers\"], config[\"cell_type\"], config[\"dropout\"])\n    model = Seq2Seq(encoder, decoder).to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    best_acc = 0\n    for epoch in range(config[\"epochs\"]):\n        train_loss = train_model(model, train_loader, optimizer, criterion, device)\n        print(f\"Epoch {epoch+1} Train Loss: {train_loss:.4f}\")\n        acc, results = evaluate_and_save(model, test_loader, input_vocab, output_vocab, device, csv_path=None)\n        if acc > best_acc:\n            best_acc = acc\n            torch.save(model.state_dict(), \"best_model.pth\")\n\n    print(\"\\n Loading best model for final evaluation...\")\n    model.load_state_dict(torch.load(\"best_model.pth\"))\n\n    # Save predictions CSV here\n    evaluate_and_save(model, test_loader, input_vocab, output_vocab, device, csv_path=\"test_predictions.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:14:40.182704Z","iopub.execute_input":"2025-05-20T12:14:40.183307Z","iopub.status.idle":"2025-05-20T12:23:59.252384Z","shell.execute_reply.started":"2025-05-20T12:14:40.183284Z","shell.execute_reply":"2025-05-20T12:23:59.251592Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 Train Loss: 1.9063\n\n Test Accuracy: 14.26%\nank             | Pred: अंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंगोकर               | Truth: अंकोर\nankor           | Pred: अंकर                 | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगरक                | Truth: अंगारक\nEpoch 2 Train Loss: 1.0576\n\n Test Accuracy: 23.79%\nank             | Pred: अंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंगकर                | Truth: अंकोर\nankor           | Pred: अंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगरक                | Truth: अंगारक\nangarak         | Pred: अंगरक                | Truth: अंगारक\nEpoch 3 Train Loss: 0.8511\n\n Test Accuracy: 27.50%\nank             | Pred: अंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अनकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: एंगकोर               | Truth: अंकोर\nankor           | Pred: अंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगरक                | Truth: अंगारक\nEpoch 4 Train Loss: 0.7406\n\n Test Accuracy: 30.76%\nank             | Pred: अंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंगकोर               | Truth: अंकोर\nankor           | Pred: अंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगरक                | Truth: अंगारक\nEpoch 5 Train Loss: 0.6723\n\n Test Accuracy: 31.27%\nank             | Pred: एंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अनकीत                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: एंगकोर               | Truth: अंकोर\nankor           | Pred: एनकोर                | Truth: अंकोर\nangaarak        | Pred: अंगरक                | Truth: अंगारक\nangarak         | Pred: अंगरक                | Truth: अंगारक\nEpoch 6 Train Loss: 0.6139\n\n Test Accuracy: 32.99%\nank             | Pred: एंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: एंगकोर               | Truth: अंकोर\nankor           | Pred: एंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगराक               | Truth: अंगारक\nEpoch 7 Train Loss: 0.5652\n\n Test Accuracy: 32.90%\nank             | Pred: अंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अनकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: एंगकोर               | Truth: अंकोर\nankor           | Pred: अंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगरक                | Truth: अंगारक\nEpoch 8 Train Loss: 0.5344\n\n Test Accuracy: 34.50%\nank             | Pred: अंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अनकीत                | Truth: अंकित\nanakon          | Pred: अनाकों               | Truth: अंकों\nankhon          | Pred: अनखों                | Truth: अंकों\nankon           | Pred: अनकों                | Truth: अंकों\nangkor          | Pred: एंगकोर               | Truth: अंकोर\nankor           | Pred: अनकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगराक               | Truth: अंगारक\nEpoch 9 Train Loss: 0.5004\n\n Test Accuracy: 33.98%\nank             | Pred: अंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अनकित                | Truth: अंकित\nanakon          | Pred: अनाकों               | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंगको                | Truth: अंकोर\nankor           | Pred: अंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगारक               | Truth: अंगारक\nEpoch 10 Train Loss: 0.4723\n\n Test Accuracy: 31.10%\nank             | Pred: एंक                  | Truth: अंक\nanka            | Pred: अंकाख                | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकोंख               | Truth: अंकों\nangkor          | Pred: एंगकोर               | Truth: अंकोर\nankor           | Pred: एंकरख                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगरक                | Truth: अंगारक\n\n Loading best model for final evaluation...\n\n Test Accuracy: 34.50%\nank             | Pred: अंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अनकीत                | Truth: अंकित\nanakon          | Pred: अनाकों               | Truth: अंकों\nankhon          | Pred: अनखों                | Truth: अंकों\nankon           | Pred: अनकों                | Truth: अंकों\nangkor          | Pred: एंगकोर               | Truth: अंकोर\nankor           | Pred: अनकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगराक               | Truth: अंगारक\n\n Predictions saved to: test_predictions.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Question-5","metadata":{}},{"cell_type":"markdown","source":"## Train the model using Attention ","metadata":{}},{"cell_type":"code","source":"# import libraries\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport pandas as pd\nimport csv\nimport wandb\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T07:10:10.493729Z","iopub.execute_input":"2025-05-20T07:10:10.494281Z","iopub.status.idle":"2025-05-20T07:10:10.498038Z","shell.execute_reply.started":"2025-05-20T07:10:10.494263Z","shell.execute_reply":"2025-05-20T07:10:10.497310Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Dataset utilities\nclass TransliterationDataset(Dataset):\n    def __init__(self, pairs, input_vocab, output_vocab):\n        self.pairs = pairs\n        self.input_vocab = input_vocab\n        self.output_vocab = output_vocab\n        self.sos = output_vocab['<sos>']\n        self.eos = output_vocab['<eos>']\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        source, target = self.pairs[idx]\n        input_ids = [self.input_vocab[c] for c in source]\n        target_ids = [self.sos] + [self.output_vocab[c] for c in target] + [self.eos]\n        return torch.tensor(input_ids), torch.tensor(target_ids)\n\ndef build_vocab(pairs):\n    input_chars = set()\n    output_chars = set()\n    for source, target in pairs:\n        input_chars.update(source)\n        output_chars.update(target)\n    input_vocab = {c: i + 1 for i, c in enumerate(sorted(input_chars))}\n    input_vocab['<pad>'] = 0\n    output_vocab = {c: i + 3 for i, c in enumerate(sorted(output_chars))}\n    output_vocab.update({'<pad>': 0, '<sos>': 1, '<eos>': 2})\n    return input_vocab, output_vocab\n\ndef load_pairs(path):\n    df = pd.read_csv(path, sep=\"\\t\", header=None, names=[\"target\", \"source\", \"count\"], dtype=str)\n    df.dropna(subset=[\"source\", \"target\"], inplace=True)\n    return list(zip(df[\"source\"], df[\"target\"]))\n\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    input_lens = [len(seq) for seq in inputs]\n    target_lens = [len(seq) for seq in targets]\n    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n    return inputs_padded, targets_padded, input_lens, target_lens\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=0)\n        rnn_class = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n        self.rnn = rnn_class(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n\n    def forward(self, x, lengths):\n        embedded = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n        outputs, hidden = self.rnn(packed)\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n        return outputs, hidden\n\nclass BahdanauAttention(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n        self.v = nn.Parameter(torch.rand(hidden_size))\n\n    def forward(self, hidden, encoder_outputs, mask):\n        timestep = encoder_outputs.size(1)\n        hidden = hidden[-1].unsqueeze(1).repeat(1, timestep, 1)\n        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n        energy = energy @ self.v\n        energy.masked_fill_(mask == 0, -1e10)\n        return torch.softmax(energy, dim=1)\n\nclass Decoder(nn.Module):\n    def __init__(self, output_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(output_size, embed_size, padding_idx=0)\n        rnn_class = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n        self.rnn = rnn_class(embed_size + hidden_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.fc = nn.Linear(hidden_size * 2, output_size)\n        self.attention = BahdanauAttention(hidden_size)\n\n    def forward(self, input_token, hidden, encoder_outputs, mask):\n        embedded = self.embedding(input_token.unsqueeze(1))\n        attn_weights = self.attention(hidden[0] if isinstance(hidden, tuple) else hidden, encoder_outputs, mask)\n        context = attn_weights.unsqueeze(1).bmm(encoder_outputs)\n        rnn_input = torch.cat([embedded, context], dim=2)\n        output, hidden = self.rnn(rnn_input, hidden)\n        output = self.fc(torch.cat([output.squeeze(1), context.squeeze(1)], dim=1))\n        return output, hidden\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def create_mask(self, src):\n        return (src != 0).float()\n\n    def forward(self, src, src_lens, tgt=None, teacher_forcing_ratio=0.5):\n        batch_size = src.size(0)\n        device = src.device\n        encoder_outputs, hidden = self.encoder(src, src_lens)\n        mask = self.create_mask(src).to(device)\n\n        if tgt is not None:\n            tgt_len = tgt.size(1)\n            outputs = torch.zeros(batch_size, tgt_len, self.decoder.fc.out_features, device=device)\n            input_token = tgt[:, 0]\n            for t in range(1, tgt_len):\n                output, hidden = self.decoder(input_token, hidden, encoder_outputs, mask)\n                outputs[:, t] = output\n                teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n                input_token = tgt[:, t] if teacher_force else output.argmax(1)\n            return outputs\n        else:\n            predictions = []\n            input_token = torch.tensor([1] * batch_size, device=device)  # <sos>\n            for _ in range(20):\n                output, hidden = self.decoder(input_token, hidden, encoder_outputs, mask)\n                top1 = output.argmax(1)\n                predictions.append(top1.unsqueeze(1))\n                input_token = top1\n            return torch.cat(predictions, dim=1)\n\ndef accuracy(preds, targets, pad_idx=0):\n    pred_tokens = preds.argmax(dim=-1)\n    correct = ((pred_tokens == targets) & (targets != pad_idx)).sum().item()\n    total = (targets != pad_idx).sum().item()\n    return correct / total if total > 0 else 0.0\n\ndef train(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss, total_acc = 0, 0\n    for src, tgt, src_lens, tgt_lens in tqdm(loader, desc=\"Training\", leave=False):\n        src, tgt = src.to(device), tgt.to(device)\n        optimizer.zero_grad()\n        output = model(src, src_lens, tgt)\n        loss = criterion(output[:, 1:].reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n        acc = accuracy(output[:, 1:], tgt[:, 1:])\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        total_acc += acc\n    return total_loss / len(loader), total_acc / len(loader)\n\n@torch.no_grad()\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    total_loss, total_acc = 0, 0\n    for src, tgt, src_lens, tgt_lens in tqdm(loader, desc=\"Evaluating\", leave=False):\n        src, tgt = src.to(device), tgt.to(device)\n        output = model(src, src_lens, tgt, teacher_forcing_ratio=0.0)\n        loss = criterion(output[:, 1:].reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n        acc = accuracy(output[:, 1:], tgt[:, 1:])\n        total_loss += loss.item()\n        total_acc += acc\n    return total_loss / len(loader), total_acc / len(loader)\n\ndef main():\n    wandb.init(project=\"dakshina-transliteration\", config=wandb.config)\n    config = wandb.config\n\n    def generate_run_name(cfg):\n        return f\"attn_cell:{cfg.cell_type}_embed:{cfg.embed_size}_hid:{cfg.hidden_size}_layers:{cfg.num_layers}\"\n\n    wandb.run.name = generate_run_name(config)\n    wandb.run.save()\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    train_pairs = load_pairs(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n    dev_pairs = load_pairs(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\")\n    input_vocab, output_vocab = build_vocab(train_pairs)\n    train_dataset = TransliterationDataset(train_pairs, input_vocab, output_vocab)\n    dev_dataset = TransliterationDataset(dev_pairs, input_vocab, output_vocab)\n\n    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n\n    encoder = Encoder(len(input_vocab), config.embed_size, config.hidden_size, config.num_layers, config.cell_type, config.dropout)\n    decoder = Decoder(len(output_vocab), config.embed_size, config.hidden_size, config.num_layers, config.cell_type, config.dropout)\n    model = Seq2Seq(encoder, decoder).to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    for epoch in range(10):\n        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n        val_loss, val_acc = evaluate(model, dev_loader, criterion, device)\n        wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"train_accuracy\": train_acc, \"val_loss\": val_loss, \"val_accuracy\": val_acc})\n\nif __name__ == \"__main__\":\n    sweep_config = {\n        \"method\": \"bayes\",\n        \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n        \"parameters\": {\n            \"embed_size\": {\"values\": [64, 128,256]},\n            \"hidden_size\": {\"values\": [64, 128,256]},\n            \"num_layers\": {\"values\": [1, 2,3]},\n            \"cell_type\": {\"values\": [\"GRU\", \"LSTM\"]},\n            \"dropout\": {\"values\": [0.2, 0.3]},\n            \"lr\": {\"min\": 0.0001, \"max\": 0.01},\n            \"batch_size\": {\"values\": [32, 64,128]}\n        }\n    }\n    sweep_id = wandb.sweep(sweep_config, project=\"dakshina-transliteration\")\n    wandb.agent(sweep_id, function=main, count=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T06:18:46.344267Z","iopub.execute_input":"2025-05-20T06:18:46.344581Z","iopub.status.idle":"2025-05-20T07:10:10.492496Z","shell.execute_reply.started":"2025-05-20T06:18:46.344527Z","shell.execute_reply":"2025-05-20T07:10:10.491979Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: p3jlr9xp\nSweep URL: https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xrjjuvjl with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0024039693612721807\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_061855-xrjjuvjl</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/xrjjuvjl' target=\"_blank\">copper-sweep-1</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/xrjjuvjl' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/xrjjuvjl</a>"},"metadata":{}},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▆▇▇██████</td></tr><tr><td>train_loss</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▇▇▇██▇▇█</td></tr><tr><td>val_loss</td><td>█▄▂▃▁▂▂▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.82932</td></tr><tr><td>train_loss</td><td>0.55515</td></tr><tr><td>val_accuracy</td><td>0.722</td></tr><tr><td>val_loss</td><td>1.05074</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">attn_cell:GRU_embed:256_hid:256_layers:3</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/xrjjuvjl' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/xrjjuvjl</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_061855-xrjjuvjl/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tx9l8q64 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.004315551903755802\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_062222-tx9l8q64</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/tx9l8q64' target=\"_blank\">fancy-sweep-2</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/tx9l8q64' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/tx9l8q64</a>"},"metadata":{}},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▆▇▇██████</td></tr><tr><td>train_loss</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▅▇█▆█▆▇▇</td></tr><tr><td>val_loss</td><td>█▇▅▁▁▄▃▃▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.77719</td></tr><tr><td>train_loss</td><td>0.72185</td></tr><tr><td>val_accuracy</td><td>0.68518</td></tr><tr><td>val_loss</td><td>1.14248</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">attn_cell:GRU_embed:256_hid:128_layers:2</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/tx9l8q64' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/tx9l8q64</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_062222-tx9l8q64/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n2hyalcl with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.007397923350248595\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_062729-n2hyalcl</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/n2hyalcl' target=\"_blank\">sleek-sweep-3</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/n2hyalcl' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/n2hyalcl</a>"},"metadata":{}},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▆▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▇▇█▇▇█▇</td></tr><tr><td>val_loss</td><td>▇▆█▂▁▁▂▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.73357</td></tr><tr><td>train_loss</td><td>0.85931</td></tr><tr><td>val_accuracy</td><td>0.66579</td></tr><tr><td>val_loss</td><td>1.13353</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">attn_cell:GRU_embed:128_hid:64_layers:2</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/n2hyalcl' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/n2hyalcl</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_062729-n2hyalcl/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: naxu7go1 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0033643846017949227\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_063240-naxu7go1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/naxu7go1' target=\"_blank\">unique-sweep-4</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/naxu7go1' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/naxu7go1</a>"},"metadata":{}},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▆▇███████</td></tr><tr><td>train_loss</td><td>█▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▅█▇▆▆▆▆</td></tr><tr><td>val_loss</td><td>█▆▄▅▁▅▄▃▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.79514</td></tr><tr><td>train_loss</td><td>0.66102</td></tr><tr><td>val_accuracy</td><td>0.69355</td></tr><tr><td>val_loss</td><td>1.10055</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">attn_cell:GRU_embed:128_hid:256_layers:3</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/naxu7go1' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/naxu7go1</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_063240-naxu7go1/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2uwxwo1f with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0005721622135068769\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_063608-2uwxwo1f</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/2uwxwo1f' target=\"_blank\">dry-sweep-5</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/2uwxwo1f' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/2uwxwo1f</a>"},"metadata":{}},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▆▆▇▇▇██</td></tr><tr><td>val_loss</td><td>█▆▅▂▂▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.86257</td></tr><tr><td>train_loss</td><td>0.45505</td></tr><tr><td>val_accuracy</td><td>0.74523</td></tr><tr><td>val_loss</td><td>0.97213</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">attn_cell:GRU_embed:256_hid:256_layers:3</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/2uwxwo1f' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/2uwxwo1f</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_063608-2uwxwo1f/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yv6y8g8z with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.00031067238358012357\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_063937-yv6y8g8z</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/yv6y8g8z' target=\"_blank\">olive-sweep-6</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/yv6y8g8z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/yv6y8g8z</a>"},"metadata":{}},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▅▆▇▇▇██</td></tr><tr><td>val_loss</td><td>█▅▄▄▃▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.78722</td></tr><tr><td>train_loss</td><td>0.70512</td></tr><tr><td>val_accuracy</td><td>0.68901</td></tr><tr><td>val_loss</td><td>1.09169</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:64_hid:64_layers:1</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/yv6y8g8z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/yv6y8g8z</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_063937-yv6y8g8z/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1idgae1h with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.00012976062620744582\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_064804-1idgae1h</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/1idgae1h' target=\"_blank\">silver-sweep-7</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/1idgae1h' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/1idgae1h</a>"},"metadata":{}},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.82003</td></tr><tr><td>train_loss</td><td>0.61</td></tr><tr><td>val_accuracy</td><td>0.7009</td></tr><tr><td>val_loss</td><td>1.06462</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:256_hid:256_layers:1</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/1idgae1h' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/1idgae1h</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_064804-1idgae1h/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1u1amg4i with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0017563387845007804\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_065055-1u1amg4i</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/1u1amg4i' target=\"_blank\">royal-sweep-8</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/1u1amg4i' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/1u1amg4i</a>"},"metadata":{}},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▅▆▇▇█▇▇▇</td></tr><tr><td>val_loss</td><td>█▄▃▂▁▂▁▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.85535</td></tr><tr><td>train_loss</td><td>0.47391</td></tr><tr><td>val_accuracy</td><td>0.72867</td></tr><tr><td>val_loss</td><td>1.02724</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">attn_cell:GRU_embed:256_hid:256_layers:3</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/1u1amg4i' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/1u1amg4i</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_065055-1u1amg4i/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fs5383uv with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.00136653692186903\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_065428-fs5383uv</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/fs5383uv' target=\"_blank\">usual-sweep-9</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/fs5383uv' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/fs5383uv</a>"},"metadata":{}},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▅▆▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▃▁▄▂▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.85033</td></tr><tr><td>train_loss</td><td>0.48697</td></tr><tr><td>val_accuracy</td><td>0.73162</td></tr><tr><td>val_loss</td><td>1.03159</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">attn_cell:GRU_embed:256_hid:256_layers:3</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/fs5383uv' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/fs5383uv</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_065428-fs5383uv/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bpe99cs7 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0008848320233546625\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'dakshina-transliteration' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_070035-bpe99cs7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/bpe99cs7' target=\"_blank\">warm-sweep-10</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/bpe99cs7' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/bpe99cs7</a>"},"metadata":{}},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▆▆▇█▇██</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▁▁▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.87394</td></tr><tr><td>train_loss</td><td>0.41396</td></tr><tr><td>val_accuracy</td><td>0.73437</td></tr><tr><td>val_loss</td><td>1.02732</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">attn_cell:GRU_embed:256_hid:256_layers:2</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/bpe99cs7' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/bpe99cs7</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_070035-bpe99cs7/logs</code>"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"Test-Attention","metadata":{}},{"cell_type":"code","source":"# ---------------- Dataset & Utils ----------------\nclass TransliterationDataset(Dataset):\n    def __init__(self, pairs, input_vocab, output_vocab):\n        self.pairs = pairs\n        self.input_vocab = input_vocab\n        self.output_vocab = output_vocab\n        self.sos = output_vocab['<sos>']\n        self.eos = output_vocab['<eos>']\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        source, target = self.pairs[idx]\n        input_ids = [self.input_vocab[c] for c in source]\n        target_ids = [self.sos] + [self.output_vocab[c] for c in target] + [self.eos]\n        return torch.tensor(input_ids), torch.tensor(target_ids)\n\ndef load_pairs(path):\n    df = pd.read_csv(path, sep='\\t', header=None, names=['target', 'source', 'count'], dtype=str)\n    df.dropna(subset=[\"source\", \"target\"], inplace=True)\n    return list(zip(df['source'], df['target']))\n\ndef build_vocab(pairs):\n    input_chars = set()\n    output_chars = set()\n    for src, tgt in pairs:\n        input_chars.update(src)\n        output_chars.update(tgt)\n    input_vocab = {c: i+1 for i, c in enumerate(sorted(input_chars))}\n    input_vocab['<pad>'] = 0\n    output_vocab = {c: i+3 for i, c in enumerate(sorted(output_chars))}\n    output_vocab.update({'<pad>': 0, '<sos>': 1, '<eos>': 2})\n    return input_vocab, output_vocab\n\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    input_lens = [len(x) for x in inputs]\n    target_lens = [len(x) for x in targets]\n    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n    return inputs_padded, targets_padded, input_lens, target_lens\n\n# ---------------- Bahdanau Attention ----------------\nclass BahdanauAttention(nn.Module):\n    def __init__(self, hidden_size, attn_size):\n        super().__init__()\n        self.W1 = nn.Linear(hidden_size, attn_size)\n        self.W2 = nn.Linear(hidden_size, attn_size)\n        self.V = nn.Linear(attn_size, 1)\n\n    def forward(self, hidden, encoder_outputs, mask=None):\n        # hidden: (num_layers * num_directions, batch, hidden_size) or (batch, hidden_size)\n        # encoder_outputs: (batch, seq_len, hidden_size)\n        # We'll take hidden from last layer (batch, hidden_size)\n        if hidden.dim() == 3:\n            hidden = hidden[-1]  # take last layer, shape: (batch, hidden_size)\n        hidden_with_time_axis = hidden.unsqueeze(1)  # (batch, 1, hidden_size)\n        score = self.V(torch.tanh(self.W1(encoder_outputs) + self.W2(hidden_with_time_axis)))  # (batch, seq_len, 1)\n        attn_weights = torch.softmax(score, dim=1)  # (batch, seq_len, 1)\n        if mask is not None:\n            attn_weights = attn_weights * mask.unsqueeze(2)  # apply mask\n            attn_weights = attn_weights / (attn_weights.sum(dim=1, keepdim=True) + 1e-10)\n        context_vector = torch.sum(attn_weights * encoder_outputs, dim=1)  # (batch, hidden_size)\n        return context_vector, attn_weights.squeeze(-1)  # attn_weights shape (batch, seq_len)\n\n# ---------------- Models ----------------\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=0)\n        rnn_cls = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n        self.rnn = rnn_cls(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0, bidirectional=False)\n\n    def forward(self, x, lengths):\n        embedded = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n        packed_outputs, hidden = self.rnn(packed)\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)  # (batch, seq_len, hidden_size)\n        return outputs, hidden  # outputs for attention, hidden for decoder init\n\nclass Decoder(nn.Module):\n    def __init__(self, output_size, embed_size, hidden_size, num_layers, cell_type, dropout, attn_size):\n        super().__init__()\n        self.embedding = nn.Embedding(output_size, embed_size, padding_idx=0)\n        rnn_cls = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n        self.rnn = rnn_cls(embed_size + hidden_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.attention = BahdanauAttention(hidden_size, attn_size)\n        self.fc = nn.Linear(hidden_size * 2, output_size)  # concat context vector + rnn output\n\n    def forward(self, input_token, hidden, encoder_outputs, mask=None):\n        # input_token: (batch,), hidden: (num_layers, batch, hidden_size)\n        embedded = self.embedding(input_token).unsqueeze(1)  # (batch, 1, embed_size)\n        context_vector, attn_weights = self.attention(hidden, encoder_outputs, mask)  # (batch, hidden_size), (batch, seq_len)\n        rnn_input = torch.cat((embedded, context_vector.unsqueeze(1)), dim=-1)  # (batch, 1, embed+hidden)\n        output, hidden = self.rnn(rnn_input, hidden)  # output: (batch,1,hidden_size)\n        output = output.squeeze(1)  # (batch, hidden_size)\n        output = torch.cat((output, context_vector), dim=1)  # (batch, hidden_size*2)\n        output = self.fc(output)  # (batch, output_size)\n        return output, hidden, attn_weights\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, output_vocab):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.output_vocab = output_vocab\n        self.sos = output_vocab['<sos>']\n        self.eos = output_vocab['<eos>']\n\n    def create_mask(self, src, src_lens):\n        # mask for padding tokens in encoder outputs (batch, seq_len)\n        batch_size, seq_len = src.size()\n        mask = torch.arange(seq_len).expand(batch_size, seq_len).to(src.device) < torch.tensor(src_lens).unsqueeze(1).to(src.device)\n        return mask\n\n    def forward(self, src, src_lens, tgt=None, teacher_forcing_ratio=0.5, max_len=20):\n        batch_size = src.size(0)\n        encoder_outputs, hidden = self.encoder(src, src_lens)  # encoder_outputs (batch, seq_len, hidden), hidden (num_layers, batch, hidden)\n        mask = self.create_mask(src, src_lens)\n\n        # Initialize decoder input and outputs\n        if tgt is not None:\n            tgt_len = tgt.size(1)\n        else:\n            tgt_len = max_len\n\n        outputs = torch.zeros(batch_size, tgt_len, self.decoder.fc.out_features).to(src.device)\n        input_token = torch.tensor([self.sos] * batch_size).to(src.device)\n\n        # For LSTM hidden is tuple (h,c), for others just tensor\n        decoder_hidden = hidden\n        if isinstance(hidden, tuple):\n            decoder_hidden = (hidden[0], hidden[1])  # just keep as is\n\n        for t in range(tgt_len):\n            output, decoder_hidden, attn_weights = self.decoder(input_token, decoder_hidden, encoder_outputs, mask)\n            outputs[:, t] = output\n            teacher_force = tgt is not None and (torch.rand(1).item() < teacher_forcing_ratio)\n            if teacher_force and t + 1 < tgt_len:\n                input_token = tgt[:, t+1]\n            else:\n                input_token = output.argmax(1)\n        return outputs\n\n# ---------------- Train + Eval ----------------\ndef train_model(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    for src, tgt, src_lens, _ in dataloader:\n        src, tgt = src.to(device), tgt.to(device)\n        optimizer.zero_grad()\n        output = model(src, src_lens, tgt)\n        loss = criterion(output[:, :-1].reshape(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n\ndef evaluate_and_save(model, dataloader, input_vocab, output_vocab, device, csv_path=None):\n    model.eval()\n    inv_input_vocab = {v: k for k, v in input_vocab.items()}\n    inv_output_vocab = {v: k for k, v in output_vocab.items()}\n    correct = 0\n    total = 0\n    results = []\n\n    with torch.no_grad():\n        for src, tgt, src_lens, _ in dataloader:\n            src = src.to(device)\n            batch_size = src.size(0)\n            encoder_outputs, hidden = model.encoder(src, src_lens)\n            mask = model.create_mask(src, src_lens)\n            input_token = torch.tensor([output_vocab['<sos>']] * batch_size).to(device)\n            decoder_hidden = hidden\n            decoded_tokens = []\n\n            max_len = 20\n            for _ in range(max_len):\n                output, decoder_hidden, attn_weights = model.decoder(input_token, decoder_hidden, encoder_outputs, mask)\n                input_token = output.argmax(1)\n                decoded_tokens.append(input_token.unsqueeze(1))\n            decoded = torch.cat(decoded_tokens, dim=1)  # (batch, max_len)\n\n            for i in range(batch_size):\n                pred = ''.join([inv_output_vocab[t.item()] for t in decoded[i] if t.item() not in [output_vocab['<eos>'], 0]])\n                truth = ''.join([inv_output_vocab[t.item()] for t in tgt[i][1:-1]])\n                inp = ''.join([inv_input_vocab[t.item()] for t in src[i] if t.item() != 0])\n                results.append((inp, pred, truth))\n                if pred == truth:\n                    correct += 1\n                total += 1\n\n    acc = correct / total * 100 if total > 0 else 0\n    print(f\"\\nTest Accuracy: {acc:.2f}%\")\n    for inp, pred, truth in results[:10]:\n        print(f\"{inp:<15} | Pred: {pred:<20} | Truth: {truth}\")\n\n    if csv_path is not None:\n        with open(csv_path, mode='w', newline='', encoding='utf-8') as f:\n            writer = csv.writer(f)\n            writer.writerow(['Input', 'Prediction', 'GroundTruth'])\n            writer.writerows(results)\n        print(f\"\\nPredictions saved to: {csv_path}\")\n\n    return acc, results\n\n# ---------------- Run ----------------\nif __name__ == \"__main__\":\n    config = {\n        \"embed_size\": 256,\n        \"hidden_size\": 128,\n        \"attn_size\": 64,\n        \"num_layers\": 3,\n        \"cell_type\": \"GRU\",\n        \"dropout\": 0.3,\n        \"batch_size\": 128,\n        \"lr\": 0.0005722,\n        \"epochs\": 10,\n    }\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    train_pairs = load_pairs(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n    test_pairs = load_pairs(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\")\n    input_vocab, output_vocab = build_vocab(train_pairs)\n    train_dataset = TransliterationDataset(train_pairs, input_vocab, output_vocab)\n    test_dataset = TransliterationDataset(test_pairs, input_vocab, output_vocab)\n\n    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=collate_fn)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n    encoder = Encoder(len(input_vocab), config[\"embed_size\"], config[\"hidden_size\"],\n                      config[\"num_layers\"], config[\"cell_type\"], config[\"dropout\"])\n    decoder = Decoder(len(output_vocab), config[\"embed_size\"], config[\"hidden_size\"],\n                      config[\"num_layers\"], config[\"cell_type\"], config[\"dropout\"], config[\"attn_size\"])\n    model = Seq2Seq(encoder, decoder, output_vocab).to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    best_acc = 0\n    for epoch in range(config[\"epochs\"]):\n        train_loss = train_model(model, train_loader, optimizer, criterion, device)\n        print(f\"Epoch {epoch+1} Train Loss: {train_loss:.4f}\")\n        acc, results = evaluate_and_save(model, test_loader, input_vocab, output_vocab, device, csv_path=None)\n        if acc > best_acc:\n            best_acc = acc\n            torch.save(model.state_dict(), \"best_model.pth\")\n\n    print(\"\\nLoading best model for final evaluation...\")\n    model.load_state_dict(torch.load(\"best_model.pth\"))\n    evaluate_and_save(model, test_loader, input_vocab, output_vocab, device, csv_path=\"test_predictions.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T07:18:27.118387Z","iopub.execute_input":"2025-05-20T07:18:27.118690Z","iopub.status.idle":"2025-05-20T07:35:06.236744Z","shell.execute_reply.started":"2025-05-20T07:18:27.118668Z","shell.execute_reply":"2025-05-20T07:35:06.236075Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 Train Loss: 2.2476\n\nTest Accuracy: 12.06%\nank             | Pred: आनक                  | Truth: अंक\nanka            | Pred: आनका                 | Truth: अंक\nankit           | Pred: अनकित                | Truth: अंकित\nanakon          | Pred: आनकों                | Truth: अंकों\nankhon          | Pred: आंखों                | Truth: अंकों\nankon           | Pred: आनकों                | Truth: अंकों\nangkor          | Pred: अंगकर                | Truth: अंकोर\nankor           | Pred: अनकोर                | Truth: अंकोर\nangaarak        | Pred: अनगाक                | Truth: अंगारक\nangarak         | Pred: अनगरक                | Truth: अंगारक\nEpoch 2 Train Loss: 1.1976\n\nTest Accuracy: 17.77%\nank             | Pred: आंक                  | Truth: अंक\nanka            | Pred: आनका                 | Truth: अंक\nankit           | Pred: अनकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: आंकों                | Truth: अंकों\nangkor          | Pred: अंगकोर               | Truth: अंकोर\nankor           | Pred: अंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगराक               | Truth: अंगारक\nEpoch 3 Train Loss: 1.0136\n\nTest Accuracy: 23.81%\nank             | Pred: आंक                  | Truth: अंक\nanka            | Pred: अनका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंगकोर               | Truth: अंकोर\nankor           | Pred: अंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगरक                | Truth: अंगारक\nEpoch 4 Train Loss: 0.9226\n\nTest Accuracy: 26.99%\nank             | Pred: आंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंगकर                | Truth: अंकोर\nankor           | Pred: अंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगरक                | Truth: अंगारक\nEpoch 5 Train Loss: 0.8520\n\nTest Accuracy: 31.36%\nank             | Pred: आंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंगकोर               | Truth: अंकोर\nankor           | Pred: अंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगरक                | Truth: अंगारक\nEpoch 6 Train Loss: 0.8147\n\nTest Accuracy: 31.92%\nank             | Pred: आंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंगकोर               | Truth: अंकोर\nankor           | Pred: अंकर                 | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगरक                | Truth: अंगारक\nEpoch 7 Train Loss: 0.7589\n\nTest Accuracy: 32.67%\nank             | Pred: अंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंगकर                | Truth: अंकोर\nankor           | Pred: अंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगराक               | Truth: अंगारक\nEpoch 8 Train Loss: 0.7303\n\nTest Accuracy: 33.85%\nank             | Pred: आंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनाकों               | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंगकोर               | Truth: अंकोर\nankor           | Pred: अंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगराक               | Truth: अंगारक\nEpoch 9 Train Loss: 0.7079\n\nTest Accuracy: 34.52%\nank             | Pred: आंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनाकों               | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंगकोर               | Truth: अंकोर\nankor           | Pred: अंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगराक               | Truth: अंगारक\nangarak         | Pred: अंगरक                | Truth: अंगारक\nEpoch 10 Train Loss: 0.6760\n\nTest Accuracy: 35.01%\nank             | Pred: आंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंगकोर               | Truth: अंकोर\nankor           | Pred: अंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगरक                | Truth: अंगारक\n\nLoading best model for final evaluation...\n\nTest Accuracy: 35.01%\nank             | Pred: आंक                  | Truth: अंक\nanka            | Pred: अंका                 | Truth: अंक\nankit           | Pred: अंकित                | Truth: अंकित\nanakon          | Pred: अनकों                | Truth: अंकों\nankhon          | Pred: अंखों                | Truth: अंकों\nankon           | Pred: अंकों                | Truth: अंकों\nangkor          | Pred: अंगकोर               | Truth: अंकोर\nankor           | Pred: अंकोर                | Truth: अंकोर\nangaarak        | Pred: अंगारक               | Truth: अंगारक\nangarak         | Pred: अंगरक                | Truth: अंगारक\n\nPredictions saved to: test_predictions.csv\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Heatmap","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport pandas as pd\nimport csv\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport math\n\n# ---------------- Dataset & Utils ----------------\nclass TransliterationDataset(Dataset):\n    def __init__(self, pairs, input_vocab, output_vocab):\n        self.pairs = pairs\n        self.input_vocab = input_vocab\n        self.output_vocab = output_vocab\n        self.sos = output_vocab['<sos>']\n        self.eos = output_vocab['<eos>']\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        source, target = self.pairs[idx]\n        input_ids = [self.input_vocab[c] for c in source]\n        target_ids = [self.sos] + [self.output_vocab[c] for c in target] + [self.eos]\n        return torch.tensor(input_ids), torch.tensor(target_ids)\n\ndef load_pairs(path):\n    df = pd.read_csv(path, sep='\\t', header=None, names=['target', 'source', 'count'], dtype=str)\n    df.dropna(subset=[\"source\", \"target\"], inplace=True)\n    return list(zip(df['source'], df['target']))\n\ndef build_vocab(pairs):\n    input_chars = set()\n    output_chars = set()\n    for src, tgt in pairs:\n        input_chars.update(src)\n        output_chars.update(tgt)\n    input_vocab = {c: i+1 for i, c in enumerate(sorted(input_chars))}\n    input_vocab['<pad>'] = 0\n    output_vocab = {c: i+3 for i, c in enumerate(sorted(output_chars))}\n    output_vocab.update({'<pad>': 0, '<sos>': 1, '<eos>': 2})\n    return input_vocab, output_vocab\n\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    input_lens = [len(x) for x in inputs]\n    target_lens = [len(x) for x in targets]\n    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n    return inputs_padded, targets_padded, input_lens, target_lens\n\n# ---------------- Bahdanau Attention ----------------\nclass BahdanauAttention(nn.Module):\n    def __init__(self, hidden_size, attn_size):\n        super().__init__()\n        self.W1 = nn.Linear(hidden_size, attn_size)\n        self.W2 = nn.Linear(hidden_size, attn_size)\n        self.V = nn.Linear(attn_size, 1)\n\n    def forward(self, hidden, encoder_outputs, mask=None):\n        # hidden: (num_layers * num_directions, batch, hidden_size) or (batch, hidden_size)\n        # encoder_outputs: (batch, seq_len, hidden_size)\n        # We'll take hidden from last layer (batch, hidden_size)\n        if hidden.dim() == 3:\n            hidden = hidden[-1]  # take last layer, shape: (batch, hidden_size)\n        hidden_with_time_axis = hidden.unsqueeze(1)  # (batch, 1, hidden_size)\n        score = self.V(torch.tanh(self.W1(encoder_outputs) + self.W2(hidden_with_time_axis)))  # (batch, seq_len, 1)\n        attn_weights = torch.softmax(score, dim=1)  # (batch, seq_len, 1)\n        if mask is not None:\n            attn_weights = attn_weights * mask.unsqueeze(2)  # apply mask\n            attn_weights = attn_weights / (attn_weights.sum(dim=1, keepdim=True) + 1e-10)\n        context_vector = torch.sum(attn_weights * encoder_outputs, dim=1)  # (batch, hidden_size)\n        return context_vector, attn_weights.squeeze(-1)  # attn_weights shape (batch, seq_len)\n\n# ---------------- Models ----------------\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=0)\n        rnn_cls = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n        self.rnn = rnn_cls(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0, bidirectional=False)\n\n    def forward(self, x, lengths):\n        embedded = self.embedding(x)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n        packed_outputs, hidden = self.rnn(packed)\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)  # (batch, seq_len, hidden_size)\n        return outputs, hidden  # outputs for attention, hidden for decoder init\n\nclass Decoder(nn.Module):\n    def __init__(self, output_size, embed_size, hidden_size, num_layers, cell_type, dropout, attn_size):\n        super().__init__()\n        self.embedding = nn.Embedding(output_size, embed_size, padding_idx=0)\n        rnn_cls = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n        self.rnn = rnn_cls(embed_size + hidden_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.attention = BahdanauAttention(hidden_size, attn_size)\n        self.fc = nn.Linear(hidden_size * 2, output_size)  # concat context vector + rnn output\n\n    def forward(self, input_token, hidden, encoder_outputs, mask=None):\n        # input_token: (batch,), hidden: (num_layers, batch, hidden_size)\n        embedded = self.embedding(input_token).unsqueeze(1)  # (batch, 1, embed_size)\n        context_vector, attn_weights = self.attention(hidden, encoder_outputs, mask)  # (batch, hidden_size), (batch, seq_len)\n        rnn_input = torch.cat((embedded, context_vector.unsqueeze(1)), dim=-1)  # (batch, 1, embed+hidden)\n        output, hidden = self.rnn(rnn_input, hidden)  # output: (batch,1,hidden_size)\n        output = output.squeeze(1)  # (batch, hidden_size)\n        output = torch.cat((output, context_vector), dim=1)  # (batch, hidden_size*2)\n        output = self.fc(output)  # (batch, output_size)\n        return output, hidden, attn_weights\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, output_vocab):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.output_vocab = output_vocab\n        self.sos = output_vocab['<sos>']\n        self.eos = output_vocab['<eos>']\n\n    def create_mask(self, src, src_lens):\n        # mask for padding tokens in encoder outputs (batch, seq_len)\n        batch_size, seq_len = src.size()\n        mask = torch.arange(seq_len).expand(batch_size, seq_len).to(src.device) < torch.tensor(src_lens).unsqueeze(1).to(src.device)\n        return mask\n\n    def forward(self, src, src_lens, tgt=None, teacher_forcing_ratio=0.5, max_len=20):\n        batch_size = src.size(0)\n        encoder_outputs, hidden = self.encoder(src, src_lens)  # encoder_outputs (batch, seq_len, hidden), hidden (num_layers, batch, hidden)\n        mask = self.create_mask(src, src_lens)\n\n        # Initialize decoder input and outputs\n        if tgt is not None:\n            tgt_len = tgt.size(1)\n        else:\n            tgt_len = max_len\n\n        outputs = torch.zeros(batch_size, tgt_len, self.decoder.fc.out_features).to(src.device)\n        input_token = torch.tensor([self.sos] * batch_size).to(src.device)\n\n        # For LSTM hidden is tuple (h,c), for others just tensor\n        decoder_hidden = hidden\n        if isinstance(hidden, tuple):\n            decoder_hidden = (hidden[0], hidden[1])  # just keep as is\n\n        for t in range(tgt_len):\n            output, decoder_hidden, attn_weights = self.decoder(input_token, decoder_hidden, encoder_outputs, mask)\n            outputs[:, t] = output\n            teacher_force = tgt is not None and (torch.rand(1).item() < teacher_forcing_ratio)\n            if teacher_force and t + 1 < tgt_len:\n                input_token = tgt[:, t+1]\n            else:\n                input_token = output.argmax(1)\n        return outputs\n\n# ---------------- New Attention Heatmap Visualization ----------------\ndef generate_attention_heatmap(model, test_loader, input_vocab, output_vocab, device, num_examples=9):\n    model.eval()\n    inv_input_vocab = {v: k for k, v in input_vocab.items()}\n    inv_output_vocab = {v: k for k, v in output_vocab.items()}\n    \n    examples = []\n    \n    with torch.no_grad():\n        for src, tgt, src_lens, _ in test_loader:\n            if len(examples) >= num_examples:\n                break\n                \n            src = src.to(device)\n            batch_size = src.size(0)\n            encoder_outputs, hidden = model.encoder(src, src_lens)\n            mask = model.create_mask(src, src_lens)\n            \n            # Get input text\n            input_text = ''.join([inv_input_vocab[t.item()] for t in src[0] if t.item() != 0])\n            \n            # Get target text\n            target_text = ''.join([inv_output_vocab[t.item()] for t in tgt[0][1:-1]])\n            \n            # Initialize for decoding\n            input_token = torch.tensor([output_vocab['<sos>']]).to(device)\n            decoder_hidden = hidden\n            \n            # Store attention weights and predicted tokens\n            attention_weights = []\n            predicted_text = []\n            \n            # Decode\n            max_len = 20\n            for _ in range(max_len):\n                output, decoder_hidden, attn_weights = model.decoder(input_token, decoder_hidden, encoder_outputs, mask)\n                attention_weights.append(attn_weights[0].cpu().numpy())\n                \n                input_token = output.argmax(1)\n                pred_token = inv_output_vocab.get(input_token.item(), '<unk>')\n                \n                if pred_token == '<eos>' or len(predicted_text) >= max_len:\n                    break\n                    \n                if pred_token not in ['<sos>', '<pad>']:\n                    predicted_text.append(pred_token)\n            \n            predicted_text = ''.join(predicted_text)\n            attention_matrix = np.array(attention_weights)\n            \n            # Add to examples\n            examples.append({\n                'input': input_text,\n                'target': target_text,\n                'prediction': predicted_text,\n                'attention': attention_matrix[:len(predicted_text), :len(input_text)]\n            })\n    \n    # Plot attention heatmaps in a grid\n    if examples:\n        num_examples = min(9, len(examples))\n        rows, cols = 3, 3  # Fixed 3x3 grid\n        \n        fig = plt.figure(figsize=(18, 18))\n        \n        for i, example in enumerate(examples[:num_examples]):\n            if i >= rows * cols:\n                break\n                \n            # Create subplot with more space for title\n            ax = fig.add_subplot(rows, cols, i + 1)\n            \n            # Add title with input, target and prediction\n            title = f\"Input: {example['input']}\\nTarget: {example['target']}\\nPred: {example['prediction']}\"\n            ax.set_title(title, fontsize=12, pad=10)\n            \n            # Generate attention heatmap\n            attention_data = example['attention']\n            \n            # Create heatmap with improved appearance\n            sns.heatmap(\n                attention_data, \n                ax=ax,\n                xticklabels=list(example['input']),\n                yticklabels=['' for _ in range(len(example['prediction']))],  # Empty y-labels like in your example\n                cmap='viridis',  # Similar to your example\n                vmin=0.0,        # Minimum value\n                vmax=1.0,        # Maximum value\n                square=True,     # Square cells\n                cbar_kws={'shrink': 0.8}  # Colorbar settings\n            )\n            \n            # Enhance the appearance of the axis labels\n            plt.setp(ax.get_xticklabels(), fontsize=12, rotation=0)\n            plt.setp(ax.get_yticklabels(), fontsize=12, rotation=0)\n            \n            # Add colorbar ticks\n            colorbar = ax.collections[0].colorbar\n            colorbar.set_ticks([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])\n            colorbar.set_ticklabels(['0.0', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8'])\n                \n        plt.tight_layout(pad=3.0)  # Add padding between subplots\n        plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n        plt.close()\n        \n        print(f\"Generated attention heatmaps for {num_examples} examples\")\n        return examples\n    else:\n        print(\"No examples found\")\n        return []\n\n# ---------------- Run ----------------\nif __name__ == \"__main__\":\n    config = {\n        \"embed_size\": 256,\n        \"hidden_size\": 128,\n        \"attn_size\": 64,\n        \"num_layers\": 3,\n        \"cell_type\": \"GRU\",\n        \"dropout\": 0.3,\n        \"batch_size\": 128,\n        \"lr\": 0.0005722,\n        \"epochs\": 10,\n    }\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    train_pairs = load_pairs(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n    test_pairs = load_pairs(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\")\n    input_vocab, output_vocab = build_vocab(train_pairs)\n    train_dataset = TransliterationDataset(train_pairs, input_vocab, output_vocab)\n    test_dataset = TransliterationDataset(test_pairs, input_vocab, output_vocab)\n\n    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=collate_fn)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n    encoder = Encoder(len(input_vocab), config[\"embed_size\"], config[\"hidden_size\"],\n                      config[\"num_layers\"], config[\"cell_type\"], config[\"dropout\"])\n    decoder = Decoder(len(output_vocab), config[\"embed_size\"], config[\"hidden_size\"],\n                      config[\"num_layers\"], config[\"cell_type\"], config[\"dropout\"], config[\"attn_size\"])\n    model = Seq2Seq(encoder, decoder, output_vocab).to(device)\n\n    # Load the best model\n    model.load_state_dict(torch.load(\"best_model.pth\"))\n    \n    # Generate attention heatmaps\n    print(\"Generating attention heatmaps...\")\n    examples = generate_attention_heatmap(model, test_loader, input_vocab, output_vocab, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T07:39:11.384139Z","iopub.execute_input":"2025-05-20T07:39:11.384748Z","iopub.status.idle":"2025-05-20T07:39:15.796313Z","shell.execute_reply.started":"2025-05-20T07:39:11.384721Z","shell.execute_reply":"2025-05-20T07:39:15.795707Z"}},"outputs":[{"name":"stdout","text":"Generating attention heatmaps...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2326 (\\N{DEVANAGARI LETTER KHA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2326 (\\N{DEVANAGARI LETTER KHA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2326 (\\N{DEVANAGARI LETTER KHA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2326 (\\N{DEVANAGARI LETTER KHA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2326 (\\N{DEVANAGARI LETTER KHA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.canvas.draw()\n/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  fig.canvas.draw()\n/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/346213603.py:258: UserWarning: Matplotlib currently does not support Devanagari natively.\n  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2326 (\\N{DEVANAGARI LETTER KHA}) missing from current font.\n  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n","output_type":"stream"},{"name":"stdout","text":"Generated attention heatmaps for 9 examples\n","output_type":"stream"}],"execution_count":12}]}