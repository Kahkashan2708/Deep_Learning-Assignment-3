{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T05:45:58.948165Z",
     "iopub.status.busy": "2025-05-20T05:45:58.947705Z",
     "iopub.status.idle": "2025-05-20T05:45:59.322269Z",
     "shell.execute_reply": "2025-05-20T05:45:59.321654Z",
     "shell.execute_reply.started": "2025-05-20T05:45:58.948142Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mma23c014\u001b[0m (\u001b[33mma23c014-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"fb4c8007ed0d1fb692b2279b11bb69081f2c698d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **$$Question-5$$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model using Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T07:10:10.494281Z",
     "iopub.status.busy": "2025-05-20T07:10:10.493729Z",
     "iopub.status.idle": "2025-05-20T07:10:10.498038Z",
     "shell.execute_reply": "2025-05-20T07:10:10.497310Z",
     "shell.execute_reply.started": "2025-05-20T07:10:10.494263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import csv\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T06:18:46.344581Z",
     "iopub.status.busy": "2025-05-20T06:18:46.344267Z",
     "iopub.status.idle": "2025-05-20T07:10:10.492496Z",
     "shell.execute_reply": "2025-05-20T07:10:10.491979Z",
     "shell.execute_reply.started": "2025-05-20T06:18:46.344527Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: p3jlr9xp\n",
      "Sweep URL: https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xrjjuvjl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0024039693612721807\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_061855-xrjjuvjl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/xrjjuvjl' target=\"_blank\">copper-sweep-1</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/xrjjuvjl' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/xrjjuvjl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▆▇▇██████</td></tr><tr><td>train_loss</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▇▇▇██▇▇█</td></tr><tr><td>val_loss</td><td>█▄▂▃▁▂▂▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.82932</td></tr><tr><td>train_loss</td><td>0.55515</td></tr><tr><td>val_accuracy</td><td>0.722</td></tr><tr><td>val_loss</td><td>1.05074</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:GRU_embed:256_hid:256_layers:3</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/xrjjuvjl' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/xrjjuvjl</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_061855-xrjjuvjl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tx9l8q64 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.004315551903755802\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_062222-tx9l8q64</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/tx9l8q64' target=\"_blank\">fancy-sweep-2</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/tx9l8q64' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/tx9l8q64</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▆▇▇██████</td></tr><tr><td>train_loss</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▅▇█▆█▆▇▇</td></tr><tr><td>val_loss</td><td>█▇▅▁▁▄▃▃▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.77719</td></tr><tr><td>train_loss</td><td>0.72185</td></tr><tr><td>val_accuracy</td><td>0.68518</td></tr><tr><td>val_loss</td><td>1.14248</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:GRU_embed:256_hid:128_layers:2</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/tx9l8q64' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/tx9l8q64</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_062222-tx9l8q64/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n2hyalcl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.007397923350248595\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_062729-n2hyalcl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/n2hyalcl' target=\"_blank\">sleek-sweep-3</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/n2hyalcl' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/n2hyalcl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▆▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▇▇█▇▇█▇</td></tr><tr><td>val_loss</td><td>▇▆█▂▁▁▂▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.73357</td></tr><tr><td>train_loss</td><td>0.85931</td></tr><tr><td>val_accuracy</td><td>0.66579</td></tr><tr><td>val_loss</td><td>1.13353</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:GRU_embed:128_hid:64_layers:2</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/n2hyalcl' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/n2hyalcl</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_062729-n2hyalcl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: naxu7go1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0033643846017949227\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_063240-naxu7go1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/naxu7go1' target=\"_blank\">unique-sweep-4</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/naxu7go1' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/naxu7go1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▆▇███████</td></tr><tr><td>train_loss</td><td>█▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▅█▇▆▆▆▆</td></tr><tr><td>val_loss</td><td>█▆▄▅▁▅▄▃▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.79514</td></tr><tr><td>train_loss</td><td>0.66102</td></tr><tr><td>val_accuracy</td><td>0.69355</td></tr><tr><td>val_loss</td><td>1.10055</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:GRU_embed:128_hid:256_layers:3</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/naxu7go1' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/naxu7go1</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_063240-naxu7go1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2uwxwo1f with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0005721622135068769\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_063608-2uwxwo1f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/2uwxwo1f' target=\"_blank\">dry-sweep-5</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/2uwxwo1f' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/2uwxwo1f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▆▆▇▇▇██</td></tr><tr><td>val_loss</td><td>█▆▅▂▂▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.86257</td></tr><tr><td>train_loss</td><td>0.45505</td></tr><tr><td>val_accuracy</td><td>0.74523</td></tr><tr><td>val_loss</td><td>0.97213</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:GRU_embed:256_hid:256_layers:3</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/2uwxwo1f' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/2uwxwo1f</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_063608-2uwxwo1f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yv6y8g8z with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.00031067238358012357\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_063937-yv6y8g8z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/yv6y8g8z' target=\"_blank\">olive-sweep-6</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/yv6y8g8z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/yv6y8g8z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▅▆▇▇▇██</td></tr><tr><td>val_loss</td><td>█▅▄▄▃▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.78722</td></tr><tr><td>train_loss</td><td>0.70512</td></tr><tr><td>val_accuracy</td><td>0.68901</td></tr><tr><td>val_loss</td><td>1.09169</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:64_hid:64_layers:1</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/yv6y8g8z' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/yv6y8g8z</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_063937-yv6y8g8z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1idgae1h with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.00012976062620744582\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_064804-1idgae1h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/1idgae1h' target=\"_blank\">silver-sweep-7</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/1idgae1h' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/1idgae1h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.82003</td></tr><tr><td>train_loss</td><td>0.61</td></tr><tr><td>val_accuracy</td><td>0.7009</td></tr><tr><td>val_loss</td><td>1.06462</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:LSTM_embed:256_hid:256_layers:1</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/1idgae1h' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/1idgae1h</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_064804-1idgae1h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1u1amg4i with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0017563387845007804\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_065055-1u1amg4i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/1u1amg4i' target=\"_blank\">royal-sweep-8</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/1u1amg4i' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/1u1amg4i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▅▆▇▇█▇▇▇</td></tr><tr><td>val_loss</td><td>█▄▃▂▁▂▁▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.85535</td></tr><tr><td>train_loss</td><td>0.47391</td></tr><tr><td>val_accuracy</td><td>0.72867</td></tr><tr><td>val_loss</td><td>1.02724</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:GRU_embed:256_hid:256_layers:3</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/1u1amg4i' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/1u1amg4i</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_065055-1u1amg4i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fs5383uv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.00136653692186903\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_065428-fs5383uv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/fs5383uv' target=\"_blank\">usual-sweep-9</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/fs5383uv' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/fs5383uv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▅▆▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▃▁▄▂▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.85033</td></tr><tr><td>train_loss</td><td>0.48697</td></tr><tr><td>val_accuracy</td><td>0.73162</td></tr><tr><td>val_loss</td><td>1.03159</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:GRU_embed:256_hid:256_layers:3</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/fs5383uv' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/fs5383uv</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_065428-fs5383uv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bpe99cs7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0008848320233546625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'dakshina-transliteration' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_070035-bpe99cs7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/bpe99cs7' target=\"_blank\">warm-sweep-10</a></strong> to <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/sweeps/p3jlr9xp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/bpe99cs7' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/bpe99cs7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▆▆▇█▇██</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▁▁▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.87394</td></tr><tr><td>train_loss</td><td>0.41396</td></tr><tr><td>val_accuracy</td><td>0.73437</td></tr><tr><td>val_loss</td><td>1.02732</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attn_cell:GRU_embed:256_hid:256_layers:2</strong> at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/bpe99cs7' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration/runs/bpe99cs7</a><br> View project at: <a href='https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration' target=\"_blank\">https://wandb.ai/ma23c014-indian-institute-of-technology-madras/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_070035-bpe99cs7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset utilities\n",
    "class TransliterationDataset(Dataset):\n",
    "    def __init__(self, pairs, input_vocab, output_vocab):\n",
    "        self.pairs = pairs\n",
    "        self.input_vocab = input_vocab\n",
    "        self.output_vocab = output_vocab\n",
    "        self.sos = output_vocab['<sos>']\n",
    "        self.eos = output_vocab['<eos>']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source, target = self.pairs[idx]\n",
    "        input_ids = [self.input_vocab[c] for c in source]\n",
    "        target_ids = [self.sos] + [self.output_vocab[c] for c in target] + [self.eos]\n",
    "        return torch.tensor(input_ids), torch.tensor(target_ids)\n",
    "\n",
    "def build_vocab(pairs):\n",
    "    input_chars = set()\n",
    "    output_chars = set()\n",
    "    for source, target in pairs:\n",
    "        input_chars.update(source)\n",
    "        output_chars.update(target)\n",
    "    input_vocab = {c: i + 1 for i, c in enumerate(sorted(input_chars))}\n",
    "    input_vocab['<pad>'] = 0\n",
    "    output_vocab = {c: i + 3 for i, c in enumerate(sorted(output_chars))}\n",
    "    output_vocab.update({'<pad>': 0, '<sos>': 1, '<eos>': 2})\n",
    "    return input_vocab, output_vocab\n",
    "\n",
    "def load_pairs(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\", header=None, names=[\"target\", \"source\", \"count\"], dtype=str)\n",
    "    df.dropna(subset=[\"source\", \"target\"], inplace=True)\n",
    "    return list(zip(df[\"source\"], df[\"target\"]))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    input_lens = [len(seq) for seq in inputs]\n",
    "    target_lens = [len(seq) for seq in targets]\n",
    "    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    return inputs_padded, targets_padded, input_lens, target_lens\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=0)\n",
    "        rnn_class = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n",
    "        self.rnn = rnn_class(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        outputs, hidden = self.rnn(packed)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "        return outputs, hidden\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        timestep = encoder_outputs.size(1)\n",
    "        hidden = hidden[-1].unsqueeze(1).repeat(1, timestep, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        energy = energy @ self.v\n",
    "        energy.masked_fill_(mask == 0, -1e10)\n",
    "        return torch.softmax(energy, dim=1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_size, embed_size, padding_idx=0)\n",
    "        rnn_class = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n",
    "        self.rnn = rnn_class(embed_size + hidden_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "\n",
    "    def forward(self, input_token, hidden, encoder_outputs, mask):\n",
    "        embedded = self.embedding(input_token.unsqueeze(1))\n",
    "        attn_weights = self.attention(hidden[0] if isinstance(hidden, tuple) else hidden, encoder_outputs, mask)\n",
    "        context = attn_weights.unsqueeze(1).bmm(encoder_outputs)\n",
    "        rnn_input = torch.cat([embedded, context], dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        output = self.fc(torch.cat([output.squeeze(1), context.squeeze(1)], dim=1))\n",
    "        return output, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def create_mask(self, src):\n",
    "        return (src != 0).float()\n",
    "\n",
    "    def forward(self, src, src_lens, tgt=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        device = src.device\n",
    "        encoder_outputs, hidden = self.encoder(src, src_lens)\n",
    "        mask = self.create_mask(src).to(device)\n",
    "\n",
    "        if tgt is not None:\n",
    "            tgt_len = tgt.size(1)\n",
    "            outputs = torch.zeros(batch_size, tgt_len, self.decoder.fc.out_features, device=device)\n",
    "            input_token = tgt[:, 0]\n",
    "            for t in range(1, tgt_len):\n",
    "                output, hidden = self.decoder(input_token, hidden, encoder_outputs, mask)\n",
    "                outputs[:, t] = output\n",
    "                teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "                input_token = tgt[:, t] if teacher_force else output.argmax(1)\n",
    "            return outputs\n",
    "        else:\n",
    "            predictions = []\n",
    "            input_token = torch.tensor([1] * batch_size, device=device)  # <sos>\n",
    "            for _ in range(20):\n",
    "                output, hidden = self.decoder(input_token, hidden, encoder_outputs, mask)\n",
    "                top1 = output.argmax(1)\n",
    "                predictions.append(top1.unsqueeze(1))\n",
    "                input_token = top1\n",
    "            return torch.cat(predictions, dim=1)\n",
    "\n",
    "def accuracy(preds, targets, pad_idx=0):\n",
    "    pred_tokens = preds.argmax(dim=-1)\n",
    "    correct = ((pred_tokens == targets) & (targets != pad_idx)).sum().item()\n",
    "    total = (targets != pad_idx).sum().item()\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, total_acc = 0, 0\n",
    "    for src, tgt, src_lens, tgt_lens in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, src_lens, tgt)\n",
    "        loss = criterion(output[:, 1:].reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "        acc = accuracy(output[:, 1:], tgt[:, 1:])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc\n",
    "    return total_loss / len(loader), total_acc / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, total_acc = 0, 0\n",
    "    for src, tgt, src_lens, tgt_lens in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        output = model(src, src_lens, tgt, teacher_forcing_ratio=0.0)\n",
    "        loss = criterion(output[:, 1:].reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "        acc = accuracy(output[:, 1:], tgt[:, 1:])\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc\n",
    "    return total_loss / len(loader), total_acc / len(loader)\n",
    "\n",
    "def main():\n",
    "    wandb.init(project=\"dakshina-transliteration\", config=wandb.config)\n",
    "    config = wandb.config\n",
    "\n",
    "    def generate_run_name(cfg):\n",
    "        return f\"attn_cell:{cfg.cell_type}_embed:{cfg.embed_size}_hid:{cfg.hidden_size}_layers:{cfg.num_layers}\"\n",
    "\n",
    "    wandb.run.name = generate_run_name(config)\n",
    "    wandb.run.save()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_pairs = load_pairs(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n",
    "    dev_pairs = load_pairs(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\")\n",
    "    input_vocab, output_vocab = build_vocab(train_pairs)\n",
    "    train_dataset = TransliterationDataset(train_pairs, input_vocab, output_vocab)\n",
    "    dev_dataset = TransliterationDataset(dev_pairs, input_vocab, output_vocab)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    encoder = Encoder(len(input_vocab), config.embed_size, config.hidden_size, config.num_layers, config.cell_type, config.dropout)\n",
    "    decoder = Decoder(len(output_vocab), config.embed_size, config.hidden_size, config.num_layers, config.cell_type, config.dropout)\n",
    "    model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = evaluate(model, dev_loader, criterion, device)\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"train_accuracy\": train_acc, \"val_loss\": val_loss, \"val_accuracy\": val_acc})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sweep_config = {\n",
    "        \"method\": \"bayes\",\n",
    "        \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "        \"parameters\": {\n",
    "            \"embed_size\": {\"values\": [64, 128,256]},\n",
    "            \"hidden_size\": {\"values\": [64, 128,256]},\n",
    "            \"num_layers\": {\"values\": [1, 2,3]},\n",
    "            \"cell_type\": {\"values\": [\"GRU\", \"LSTM\"]},\n",
    "            \"dropout\": {\"values\": [0.2, 0.3]},\n",
    "            \"lr\": {\"min\": 0.0001, \"max\": 0.01},\n",
    "            \"batch_size\": {\"values\": [32, 64,128]}\n",
    "        }\n",
    "    }\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"dakshina-transliteration\")\n",
    "    wandb.agent(sweep_id, function=main, count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T07:18:27.118690Z",
     "iopub.status.busy": "2025-05-20T07:18:27.118387Z",
     "iopub.status.idle": "2025-05-20T07:35:06.236744Z",
     "shell.execute_reply": "2025-05-20T07:35:06.236075Z",
     "shell.execute_reply.started": "2025-05-20T07:18:27.118668Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Loss: 2.2476\n",
      "\n",
      "Test Accuracy: 12.06%\n",
      "ank             | Pred: आनक                  | Truth: अंक\n",
      "anka            | Pred: आनका                 | Truth: अंक\n",
      "ankit           | Pred: अनकित                | Truth: अंकित\n",
      "anakon          | Pred: आनकों                | Truth: अंकों\n",
      "ankhon          | Pred: आंखों                | Truth: अंकों\n",
      "ankon           | Pred: आनकों                | Truth: अंकों\n",
      "angkor          | Pred: अंगकर                | Truth: अंकोर\n",
      "ankor           | Pred: अनकोर                | Truth: अंकोर\n",
      "angaarak        | Pred: अनगाक                | Truth: अंगारक\n",
      "angarak         | Pred: अनगरक                | Truth: अंगारक\n",
      "Epoch 2 Train Loss: 1.1976\n",
      "\n",
      "Test Accuracy: 17.77%\n",
      "ank             | Pred: आंक                  | Truth: अंक\n",
      "anka            | Pred: आनका                 | Truth: अंक\n",
      "ankit           | Pred: अनकित                | Truth: अंकित\n",
      "anakon          | Pred: अनकों                | Truth: अंकों\n",
      "ankhon          | Pred: अंखों                | Truth: अंकों\n",
      "ankon           | Pred: आंकों                | Truth: अंकों\n",
      "angkor          | Pred: अंगकोर               | Truth: अंकोर\n",
      "ankor           | Pred: अंकोर                | Truth: अंकोर\n",
      "angaarak        | Pred: अंगारक               | Truth: अंगारक\n",
      "angarak         | Pred: अंगराक               | Truth: अंगारक\n",
      "Epoch 3 Train Loss: 1.0136\n",
      "\n",
      "Test Accuracy: 23.81%\n",
      "ank             | Pred: आंक                  | Truth: अंक\n",
      "anka            | Pred: अनका                 | Truth: अंक\n",
      "ankit           | Pred: अंकित                | Truth: अंकित\n",
      "anakon          | Pred: अनकों                | Truth: अंकों\n",
      "ankhon          | Pred: अंखों                | Truth: अंकों\n",
      "ankon           | Pred: अंकों                | Truth: अंकों\n",
      "angkor          | Pred: अंगकोर               | Truth: अंकोर\n",
      "ankor           | Pred: अंकोर                | Truth: अंकोर\n",
      "angaarak        | Pred: अंगारक               | Truth: अंगारक\n",
      "angarak         | Pred: अंगरक                | Truth: अंगारक\n",
      "Epoch 4 Train Loss: 0.9226\n",
      "\n",
      "Test Accuracy: 26.99%\n",
      "ank             | Pred: आंक                  | Truth: अंक\n",
      "anka            | Pred: अंका                 | Truth: अंक\n",
      "ankit           | Pred: अंकित                | Truth: अंकित\n",
      "anakon          | Pred: अनकों                | Truth: अंकों\n",
      "ankhon          | Pred: अंखों                | Truth: अंकों\n",
      "ankon           | Pred: अंकों                | Truth: अंकों\n",
      "angkor          | Pred: अंगकर                | Truth: अंकोर\n",
      "ankor           | Pred: अंकोर                | Truth: अंकोर\n",
      "angaarak        | Pred: अंगारक               | Truth: अंगारक\n",
      "angarak         | Pred: अंगरक                | Truth: अंगारक\n",
      "Epoch 5 Train Loss: 0.8520\n",
      "\n",
      "Test Accuracy: 31.36%\n",
      "ank             | Pred: आंक                  | Truth: अंक\n",
      "anka            | Pred: अंका                 | Truth: अंक\n",
      "ankit           | Pred: अंकित                | Truth: अंकित\n",
      "anakon          | Pred: अनकों                | Truth: अंकों\n",
      "ankhon          | Pred: अंखों                | Truth: अंकों\n",
      "ankon           | Pred: अंकों                | Truth: अंकों\n",
      "angkor          | Pred: अंगकोर               | Truth: अंकोर\n",
      "ankor           | Pred: अंकोर                | Truth: अंकोर\n",
      "angaarak        | Pred: अंगारक               | Truth: अंगारक\n",
      "angarak         | Pred: अंगरक                | Truth: अंगारक\n",
      "Epoch 6 Train Loss: 0.8147\n",
      "\n",
      "Test Accuracy: 31.92%\n",
      "ank             | Pred: आंक                  | Truth: अंक\n",
      "anka            | Pred: अंका                 | Truth: अंक\n",
      "ankit           | Pred: अंकित                | Truth: अंकित\n",
      "anakon          | Pred: अनकों                | Truth: अंकों\n",
      "ankhon          | Pred: अंखों                | Truth: अंकों\n",
      "ankon           | Pred: अंकों                | Truth: अंकों\n",
      "angkor          | Pred: अंगकोर               | Truth: अंकोर\n",
      "ankor           | Pred: अंकर                 | Truth: अंकोर\n",
      "angaarak        | Pred: अंगारक               | Truth: अंगारक\n",
      "angarak         | Pred: अंगरक                | Truth: अंगारक\n",
      "Epoch 7 Train Loss: 0.7589\n",
      "\n",
      "Test Accuracy: 32.67%\n",
      "ank             | Pred: अंक                  | Truth: अंक\n",
      "anka            | Pred: अंका                 | Truth: अंक\n",
      "ankit           | Pred: अंकित                | Truth: अंकित\n",
      "anakon          | Pred: अनकों                | Truth: अंकों\n",
      "ankhon          | Pred: अंखों                | Truth: अंकों\n",
      "ankon           | Pred: अंकों                | Truth: अंकों\n",
      "angkor          | Pred: अंगकर                | Truth: अंकोर\n",
      "ankor           | Pred: अंकोर                | Truth: अंकोर\n",
      "angaarak        | Pred: अंगारक               | Truth: अंगारक\n",
      "angarak         | Pred: अंगराक               | Truth: अंगारक\n",
      "Epoch 8 Train Loss: 0.7303\n",
      "\n",
      "Test Accuracy: 33.85%\n",
      "ank             | Pred: आंक                  | Truth: अंक\n",
      "anka            | Pred: अंका                 | Truth: अंक\n",
      "ankit           | Pred: अंकित                | Truth: अंकित\n",
      "anakon          | Pred: अनाकों               | Truth: अंकों\n",
      "ankhon          | Pred: अंखों                | Truth: अंकों\n",
      "ankon           | Pred: अंकों                | Truth: अंकों\n",
      "angkor          | Pred: अंगकोर               | Truth: अंकोर\n",
      "ankor           | Pred: अंकोर                | Truth: अंकोर\n",
      "angaarak        | Pred: अंगारक               | Truth: अंगारक\n",
      "angarak         | Pred: अंगराक               | Truth: अंगारक\n",
      "Epoch 9 Train Loss: 0.7079\n",
      "\n",
      "Test Accuracy: 34.52%\n",
      "ank             | Pred: आंक                  | Truth: अंक\n",
      "anka            | Pred: अंका                 | Truth: अंक\n",
      "ankit           | Pred: अंकित                | Truth: अंकित\n",
      "anakon          | Pred: अनाकों               | Truth: अंकों\n",
      "ankhon          | Pred: अंखों                | Truth: अंकों\n",
      "ankon           | Pred: अंकों                | Truth: अंकों\n",
      "angkor          | Pred: अंगकोर               | Truth: अंकोर\n",
      "ankor           | Pred: अंकोर                | Truth: अंकोर\n",
      "angaarak        | Pred: अंगराक               | Truth: अंगारक\n",
      "angarak         | Pred: अंगरक                | Truth: अंगारक\n",
      "Epoch 10 Train Loss: 0.6760\n",
      "\n",
      "Test Accuracy: 35.01%\n",
      "ank             | Pred: आंक                  | Truth: अंक\n",
      "anka            | Pred: अंका                 | Truth: अंक\n",
      "ankit           | Pred: अंकित                | Truth: अंकित\n",
      "anakon          | Pred: अनकों                | Truth: अंकों\n",
      "ankhon          | Pred: अंखों                | Truth: अंकों\n",
      "ankon           | Pred: अंकों                | Truth: अंकों\n",
      "angkor          | Pred: अंगकोर               | Truth: अंकोर\n",
      "ankor           | Pred: अंकोर                | Truth: अंकोर\n",
      "angaarak        | Pred: अंगारक               | Truth: अंगारक\n",
      "angarak         | Pred: अंगरक                | Truth: अंगारक\n",
      "\n",
      "Loading best model for final evaluation...\n",
      "\n",
      "Test Accuracy: 35.01%\n",
      "ank             | Pred: आंक                  | Truth: अंक\n",
      "anka            | Pred: अंका                 | Truth: अंक\n",
      "ankit           | Pred: अंकित                | Truth: अंकित\n",
      "anakon          | Pred: अनकों                | Truth: अंकों\n",
      "ankhon          | Pred: अंखों                | Truth: अंकों\n",
      "ankon           | Pred: अंकों                | Truth: अंकों\n",
      "angkor          | Pred: अंगकोर               | Truth: अंकोर\n",
      "ankor           | Pred: अंकोर                | Truth: अंकोर\n",
      "angaarak        | Pred: अंगारक               | Truth: अंगारक\n",
      "angarak         | Pred: अंगरक                | Truth: अंगारक\n",
      "\n",
      "Predictions saved to: test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Dataset & Utils ----------------\n",
    "class TransliterationDataset(Dataset):\n",
    "    def __init__(self, pairs, input_vocab, output_vocab):\n",
    "        self.pairs = pairs\n",
    "        self.input_vocab = input_vocab\n",
    "        self.output_vocab = output_vocab\n",
    "        self.sos = output_vocab['<sos>']\n",
    "        self.eos = output_vocab['<eos>']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source, target = self.pairs[idx]\n",
    "        input_ids = [self.input_vocab[c] for c in source]\n",
    "        target_ids = [self.sos] + [self.output_vocab[c] for c in target] + [self.eos]\n",
    "        return torch.tensor(input_ids), torch.tensor(target_ids)\n",
    "\n",
    "def load_pairs(path):\n",
    "    df = pd.read_csv(path, sep='\\t', header=None, names=['target', 'source', 'count'], dtype=str)\n",
    "    df.dropna(subset=[\"source\", \"target\"], inplace=True)\n",
    "    return list(zip(df['source'], df['target']))\n",
    "\n",
    "def build_vocab(pairs):\n",
    "    input_chars = set()\n",
    "    output_chars = set()\n",
    "    for src, tgt in pairs:\n",
    "        input_chars.update(src)\n",
    "        output_chars.update(tgt)\n",
    "    input_vocab = {c: i+1 for i, c in enumerate(sorted(input_chars))}\n",
    "    input_vocab['<pad>'] = 0\n",
    "    output_vocab = {c: i+3 for i, c in enumerate(sorted(output_chars))}\n",
    "    output_vocab.update({'<pad>': 0, '<sos>': 1, '<eos>': 2})\n",
    "    return input_vocab, output_vocab\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    input_lens = [len(x) for x in inputs]\n",
    "    target_lens = [len(x) for x in targets]\n",
    "    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    return inputs_padded, targets_padded, input_lens, target_lens\n",
    "\n",
    "# ---------------- Bahdanau Attention ----------------\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, attn_size):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(hidden_size, attn_size)\n",
    "        self.W2 = nn.Linear(hidden_size, attn_size)\n",
    "        self.V = nn.Linear(attn_size, 1)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        # hidden: (num_layers * num_directions, batch, hidden_size) or (batch, hidden_size)\n",
    "        # encoder_outputs: (batch, seq_len, hidden_size)\n",
    "        # We'll take hidden from last layer (batch, hidden_size)\n",
    "        if hidden.dim() == 3:\n",
    "            hidden = hidden[-1]  # take last layer, shape: (batch, hidden_size)\n",
    "        hidden_with_time_axis = hidden.unsqueeze(1)  # (batch, 1, hidden_size)\n",
    "        score = self.V(torch.tanh(self.W1(encoder_outputs) + self.W2(hidden_with_time_axis)))  # (batch, seq_len, 1)\n",
    "        attn_weights = torch.softmax(score, dim=1)  # (batch, seq_len, 1)\n",
    "        if mask is not None:\n",
    "            attn_weights = attn_weights * mask.unsqueeze(2)  # apply mask\n",
    "            attn_weights = attn_weights / (attn_weights.sum(dim=1, keepdim=True) + 1e-10)\n",
    "        context_vector = torch.sum(attn_weights * encoder_outputs, dim=1)  # (batch, hidden_size)\n",
    "        return context_vector, attn_weights.squeeze(-1)  # attn_weights shape (batch, seq_len)\n",
    "\n",
    "# ---------------- Models ----------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=0)\n",
    "        rnn_cls = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n",
    "        self.rnn = rnn_cls(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0, bidirectional=False)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_outputs, hidden = self.rnn(packed)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)  # (batch, seq_len, hidden_size)\n",
    "        return outputs, hidden  # outputs for attention, hidden for decoder init\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embed_size, hidden_size, num_layers, cell_type, dropout, attn_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_size, embed_size, padding_idx=0)\n",
    "        rnn_cls = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n",
    "        self.rnn = rnn_cls(embed_size + hidden_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.attention = BahdanauAttention(hidden_size, attn_size)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)  # concat context vector + rnn output\n",
    "\n",
    "    def forward(self, input_token, hidden, encoder_outputs, mask=None):\n",
    "        # input_token: (batch,), hidden: (num_layers, batch, hidden_size)\n",
    "        embedded = self.embedding(input_token).unsqueeze(1)  # (batch, 1, embed_size)\n",
    "        context_vector, attn_weights = self.attention(hidden, encoder_outputs, mask)  # (batch, hidden_size), (batch, seq_len)\n",
    "        rnn_input = torch.cat((embedded, context_vector.unsqueeze(1)), dim=-1)  # (batch, 1, embed+hidden)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)  # output: (batch,1,hidden_size)\n",
    "        output = output.squeeze(1)  # (batch, hidden_size)\n",
    "        output = torch.cat((output, context_vector), dim=1)  # (batch, hidden_size*2)\n",
    "        output = self.fc(output)  # (batch, output_size)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, output_vocab):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.output_vocab = output_vocab\n",
    "        self.sos = output_vocab['<sos>']\n",
    "        self.eos = output_vocab['<eos>']\n",
    "\n",
    "    def create_mask(self, src, src_lens):\n",
    "        # mask for padding tokens in encoder outputs (batch, seq_len)\n",
    "        batch_size, seq_len = src.size()\n",
    "        mask = torch.arange(seq_len).expand(batch_size, seq_len).to(src.device) < torch.tensor(src_lens).unsqueeze(1).to(src.device)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, src_lens, tgt=None, teacher_forcing_ratio=0.5, max_len=20):\n",
    "        batch_size = src.size(0)\n",
    "        encoder_outputs, hidden = self.encoder(src, src_lens)  # encoder_outputs (batch, seq_len, hidden), hidden (num_layers, batch, hidden)\n",
    "        mask = self.create_mask(src, src_lens)\n",
    "\n",
    "        # Initialize decoder input and outputs\n",
    "        if tgt is not None:\n",
    "            tgt_len = tgt.size(1)\n",
    "        else:\n",
    "            tgt_len = max_len\n",
    "\n",
    "        outputs = torch.zeros(batch_size, tgt_len, self.decoder.fc.out_features).to(src.device)\n",
    "        input_token = torch.tensor([self.sos] * batch_size).to(src.device)\n",
    "\n",
    "        # For LSTM hidden is tuple (h,c), for others just tensor\n",
    "        decoder_hidden = hidden\n",
    "        if isinstance(hidden, tuple):\n",
    "            decoder_hidden = (hidden[0], hidden[1])  # just keep as is\n",
    "\n",
    "        for t in range(tgt_len):\n",
    "            output, decoder_hidden, attn_weights = self.decoder(input_token, decoder_hidden, encoder_outputs, mask)\n",
    "            outputs[:, t] = output\n",
    "            teacher_force = tgt is not None and (torch.rand(1).item() < teacher_forcing_ratio)\n",
    "            if teacher_force and t + 1 < tgt_len:\n",
    "                input_token = tgt[:, t+1]\n",
    "            else:\n",
    "                input_token = output.argmax(1)\n",
    "        return outputs\n",
    "\n",
    "# ---------------- Train + Eval ----------------\n",
    "def train_model(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt, src_lens, _ in dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, src_lens, tgt)\n",
    "        loss = criterion(output[:, :-1].reshape(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_and_save(model, dataloader, input_vocab, output_vocab, device, csv_path=None):\n",
    "    model.eval()\n",
    "    inv_input_vocab = {v: k for k, v in input_vocab.items()}\n",
    "    inv_output_vocab = {v: k for k, v in output_vocab.items()}\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt, src_lens, _ in dataloader:\n",
    "            src = src.to(device)\n",
    "            batch_size = src.size(0)\n",
    "            encoder_outputs, hidden = model.encoder(src, src_lens)\n",
    "            mask = model.create_mask(src, src_lens)\n",
    "            input_token = torch.tensor([output_vocab['<sos>']] * batch_size).to(device)\n",
    "            decoder_hidden = hidden\n",
    "            decoded_tokens = []\n",
    "\n",
    "            max_len = 20\n",
    "            for _ in range(max_len):\n",
    "                output, decoder_hidden, attn_weights = model.decoder(input_token, decoder_hidden, encoder_outputs, mask)\n",
    "                input_token = output.argmax(1)\n",
    "                decoded_tokens.append(input_token.unsqueeze(1))\n",
    "            decoded = torch.cat(decoded_tokens, dim=1)  # (batch, max_len)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                pred = ''.join([inv_output_vocab[t.item()] for t in decoded[i] if t.item() not in [output_vocab['<eos>'], 0]])\n",
    "                truth = ''.join([inv_output_vocab[t.item()] for t in tgt[i][1:-1]])\n",
    "                inp = ''.join([inv_input_vocab[t.item()] for t in src[i] if t.item() != 0])\n",
    "                results.append((inp, pred, truth))\n",
    "                if pred == truth:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "    acc = correct / total * 100 if total > 0 else 0\n",
    "    print(f\"\\nTest Accuracy: {acc:.2f}%\")\n",
    "    for inp, pred, truth in results[:10]:\n",
    "        print(f\"{inp:<15} | Pred: {pred:<20} | Truth: {truth}\")\n",
    "\n",
    "    if csv_path is not None:\n",
    "        with open(csv_path, mode='w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['Input', 'Prediction', 'GroundTruth'])\n",
    "            writer.writerows(results)\n",
    "        print(f\"\\nPredictions saved to: {csv_path}\")\n",
    "\n",
    "    return acc, results\n",
    "\n",
    "# ---------------- Run ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        \"embed_size\": 256,\n",
    "        \"hidden_size\": 128,\n",
    "        \"attn_size\": 64,\n",
    "        \"num_layers\": 3,\n",
    "        \"cell_type\": \"GRU\",\n",
    "        \"dropout\": 0.3,\n",
    "        \"batch_size\": 128,\n",
    "        \"lr\": 0.0005722,\n",
    "        \"epochs\": 10,\n",
    "    }\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_pairs = load_pairs(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n",
    "    test_pairs = load_pairs(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\")\n",
    "    input_vocab, output_vocab = build_vocab(train_pairs)\n",
    "    train_dataset = TransliterationDataset(train_pairs, input_vocab, output_vocab)\n",
    "    test_dataset = TransliterationDataset(test_pairs, input_vocab, output_vocab)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    encoder = Encoder(len(input_vocab), config[\"embed_size\"], config[\"hidden_size\"],\n",
    "                      config[\"num_layers\"], config[\"cell_type\"], config[\"dropout\"])\n",
    "    decoder = Decoder(len(output_vocab), config[\"embed_size\"], config[\"hidden_size\"],\n",
    "                      config[\"num_layers\"], config[\"cell_type\"], config[\"dropout\"], config[\"attn_size\"])\n",
    "    model = Seq2Seq(encoder, decoder, output_vocab).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    best_acc = 0\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "        print(f\"Epoch {epoch+1} Train Loss: {train_loss:.4f}\")\n",
    "        acc, results = evaluate_and_save(model, test_loader, input_vocab, output_vocab, device, csv_path=None)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    print(\"\\nLoading best model for final evaluation...\")\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    evaluate_and_save(model, test_loader, input_vocab, output_vocab, device, csv_path=\"test_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T07:39:11.384748Z",
     "iopub.status.busy": "2025-05-20T07:39:11.384139Z",
     "iopub.status.idle": "2025-05-20T07:39:15.796313Z",
     "shell.execute_reply": "2025-05-20T07:39:15.795707Z",
     "shell.execute_reply.started": "2025-05-20T07:39:11.384721Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating attention heatmaps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2326 (\\N{DEVANAGARI LETTER KHA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2326 (\\N{DEVANAGARI LETTER KHA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2326 (\\N{DEVANAGARI LETTER KHA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2326 (\\N{DEVANAGARI LETTER KHA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Devanagari natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2326 (\\N{DEVANAGARI LETTER KHA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/346213603.py:258: UserWarning: Matplotlib currently does not support Devanagari natively.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2310 (\\N{DEVANAGARI LETTER AA}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2344 (\\N{DEVANAGARI LETTER NA}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2326 (\\N{DEVANAGARI LETTER KHA}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_35/346213603.py:258: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n",
      "  plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated attention heatmaps for 9 examples\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Dataset & Utils ----------------\n",
    "class TransliterationDataset(Dataset):\n",
    "    def __init__(self, pairs, input_vocab, output_vocab):\n",
    "        self.pairs = pairs\n",
    "        self.input_vocab = input_vocab\n",
    "        self.output_vocab = output_vocab\n",
    "        self.sos = output_vocab['<sos>']\n",
    "        self.eos = output_vocab['<eos>']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source, target = self.pairs[idx]\n",
    "        input_ids = [self.input_vocab[c] for c in source]\n",
    "        target_ids = [self.sos] + [self.output_vocab[c] for c in target] + [self.eos]\n",
    "        return torch.tensor(input_ids), torch.tensor(target_ids)\n",
    "\n",
    "def load_pairs(path):\n",
    "    df = pd.read_csv(path, sep='\\t', header=None, names=['target', 'source', 'count'], dtype=str)\n",
    "    df.dropna(subset=[\"source\", \"target\"], inplace=True)\n",
    "    return list(zip(df['source'], df['target']))\n",
    "\n",
    "def build_vocab(pairs):\n",
    "    input_chars = set()\n",
    "    output_chars = set()\n",
    "    for src, tgt in pairs:\n",
    "        input_chars.update(src)\n",
    "        output_chars.update(tgt)\n",
    "    input_vocab = {c: i+1 for i, c in enumerate(sorted(input_chars))}\n",
    "    input_vocab['<pad>'] = 0\n",
    "    output_vocab = {c: i+3 for i, c in enumerate(sorted(output_chars))}\n",
    "    output_vocab.update({'<pad>': 0, '<sos>': 1, '<eos>': 2})\n",
    "    return input_vocab, output_vocab\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    input_lens = [len(x) for x in inputs]\n",
    "    target_lens = [len(x) for x in targets]\n",
    "    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    return inputs_padded, targets_padded, input_lens, target_lens\n",
    "\n",
    "# ---------------- Bahdanau Attention ----------------\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, attn_size):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(hidden_size, attn_size)\n",
    "        self.W2 = nn.Linear(hidden_size, attn_size)\n",
    "        self.V = nn.Linear(attn_size, 1)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        # hidden: (num_layers * num_directions, batch, hidden_size) or (batch, hidden_size)\n",
    "        # encoder_outputs: (batch, seq_len, hidden_size)\n",
    "        # We'll take hidden from last layer (batch, hidden_size)\n",
    "        if hidden.dim() == 3:\n",
    "            hidden = hidden[-1]  # take last layer, shape: (batch, hidden_size)\n",
    "        hidden_with_time_axis = hidden.unsqueeze(1)  # (batch, 1, hidden_size)\n",
    "        score = self.V(torch.tanh(self.W1(encoder_outputs) + self.W2(hidden_with_time_axis)))  # (batch, seq_len, 1)\n",
    "        attn_weights = torch.softmax(score, dim=1)  # (batch, seq_len, 1)\n",
    "        if mask is not None:\n",
    "            attn_weights = attn_weights * mask.unsqueeze(2)  # apply mask\n",
    "            attn_weights = attn_weights / (attn_weights.sum(dim=1, keepdim=True) + 1e-10)\n",
    "        context_vector = torch.sum(attn_weights * encoder_outputs, dim=1)  # (batch, hidden_size)\n",
    "        return context_vector, attn_weights.squeeze(-1)  # attn_weights shape (batch, seq_len)\n",
    "\n",
    "# ---------------- Models ----------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, num_layers, cell_type, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=0)\n",
    "        rnn_cls = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n",
    "        self.rnn = rnn_cls(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0, bidirectional=False)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_outputs, hidden = self.rnn(packed)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)  # (batch, seq_len, hidden_size)\n",
    "        return outputs, hidden  # outputs for attention, hidden for decoder init\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embed_size, hidden_size, num_layers, cell_type, dropout, attn_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_size, embed_size, padding_idx=0)\n",
    "        rnn_cls = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[cell_type]\n",
    "        self.rnn = rnn_cls(embed_size + hidden_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.attention = BahdanauAttention(hidden_size, attn_size)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)  # concat context vector + rnn output\n",
    "\n",
    "    def forward(self, input_token, hidden, encoder_outputs, mask=None):\n",
    "        # input_token: (batch,), hidden: (num_layers, batch, hidden_size)\n",
    "        embedded = self.embedding(input_token).unsqueeze(1)  # (batch, 1, embed_size)\n",
    "        context_vector, attn_weights = self.attention(hidden, encoder_outputs, mask)  # (batch, hidden_size), (batch, seq_len)\n",
    "        rnn_input = torch.cat((embedded, context_vector.unsqueeze(1)), dim=-1)  # (batch, 1, embed+hidden)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)  # output: (batch,1,hidden_size)\n",
    "        output = output.squeeze(1)  # (batch, hidden_size)\n",
    "        output = torch.cat((output, context_vector), dim=1)  # (batch, hidden_size*2)\n",
    "        output = self.fc(output)  # (batch, output_size)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, output_vocab):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.output_vocab = output_vocab\n",
    "        self.sos = output_vocab['<sos>']\n",
    "        self.eos = output_vocab['<eos>']\n",
    "\n",
    "    def create_mask(self, src, src_lens):\n",
    "        # mask for padding tokens in encoder outputs (batch, seq_len)\n",
    "        batch_size, seq_len = src.size()\n",
    "        mask = torch.arange(seq_len).expand(batch_size, seq_len).to(src.device) < torch.tensor(src_lens).unsqueeze(1).to(src.device)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, src_lens, tgt=None, teacher_forcing_ratio=0.5, max_len=20):\n",
    "        batch_size = src.size(0)\n",
    "        encoder_outputs, hidden = self.encoder(src, src_lens)  # encoder_outputs (batch, seq_len, hidden), hidden (num_layers, batch, hidden)\n",
    "        mask = self.create_mask(src, src_lens)\n",
    "\n",
    "        # Initialize decoder input and outputs\n",
    "        if tgt is not None:\n",
    "            tgt_len = tgt.size(1)\n",
    "        else:\n",
    "            tgt_len = max_len\n",
    "\n",
    "        outputs = torch.zeros(batch_size, tgt_len, self.decoder.fc.out_features).to(src.device)\n",
    "        input_token = torch.tensor([self.sos] * batch_size).to(src.device)\n",
    "\n",
    "        # For LSTM hidden is tuple (h,c), for others just tensor\n",
    "        decoder_hidden = hidden\n",
    "        if isinstance(hidden, tuple):\n",
    "            decoder_hidden = (hidden[0], hidden[1])  # just keep as is\n",
    "\n",
    "        for t in range(tgt_len):\n",
    "            output, decoder_hidden, attn_weights = self.decoder(input_token, decoder_hidden, encoder_outputs, mask)\n",
    "            outputs[:, t] = output\n",
    "            teacher_force = tgt is not None and (torch.rand(1).item() < teacher_forcing_ratio)\n",
    "            if teacher_force and t + 1 < tgt_len:\n",
    "                input_token = tgt[:, t+1]\n",
    "            else:\n",
    "                input_token = output.argmax(1)\n",
    "        return outputs\n",
    "\n",
    "# ---------------- New Attention Heatmap Visualization ----------------\n",
    "def generate_attention_heatmap(model, test_loader, input_vocab, output_vocab, device, num_examples=9):\n",
    "    model.eval()\n",
    "    inv_input_vocab = {v: k for k, v in input_vocab.items()}\n",
    "    inv_output_vocab = {v: k for k, v in output_vocab.items()}\n",
    "    \n",
    "    examples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, tgt, src_lens, _ in test_loader:\n",
    "            if len(examples) >= num_examples:\n",
    "                break\n",
    "                \n",
    "            src = src.to(device)\n",
    "            batch_size = src.size(0)\n",
    "            encoder_outputs, hidden = model.encoder(src, src_lens)\n",
    "            mask = model.create_mask(src, src_lens)\n",
    "            \n",
    "            # Get input text\n",
    "            input_text = ''.join([inv_input_vocab[t.item()] for t in src[0] if t.item() != 0])\n",
    "            \n",
    "            # Get target text\n",
    "            target_text = ''.join([inv_output_vocab[t.item()] for t in tgt[0][1:-1]])\n",
    "            \n",
    "            # Initialize for decoding\n",
    "            input_token = torch.tensor([output_vocab['<sos>']]).to(device)\n",
    "            decoder_hidden = hidden\n",
    "            \n",
    "            # Store attention weights and predicted tokens\n",
    "            attention_weights = []\n",
    "            predicted_text = []\n",
    "            \n",
    "            # Decode\n",
    "            max_len = 20\n",
    "            for _ in range(max_len):\n",
    "                output, decoder_hidden, attn_weights = model.decoder(input_token, decoder_hidden, encoder_outputs, mask)\n",
    "                attention_weights.append(attn_weights[0].cpu().numpy())\n",
    "                \n",
    "                input_token = output.argmax(1)\n",
    "                pred_token = inv_output_vocab.get(input_token.item(), '<unk>')\n",
    "                \n",
    "                if pred_token == '<eos>' or len(predicted_text) >= max_len:\n",
    "                    break\n",
    "                    \n",
    "                if pred_token not in ['<sos>', '<pad>']:\n",
    "                    predicted_text.append(pred_token)\n",
    "            \n",
    "            predicted_text = ''.join(predicted_text)\n",
    "            attention_matrix = np.array(attention_weights)\n",
    "            \n",
    "            # Add to examples\n",
    "            examples.append({\n",
    "                'input': input_text,\n",
    "                'target': target_text,\n",
    "                'prediction': predicted_text,\n",
    "                'attention': attention_matrix[:len(predicted_text), :len(input_text)]\n",
    "            })\n",
    "    \n",
    "    # Plot attention heatmaps in a grid\n",
    "    if examples:\n",
    "        num_examples = min(9, len(examples))\n",
    "        rows, cols = 3, 3  # Fixed 3x3 grid\n",
    "        \n",
    "        fig = plt.figure(figsize=(18, 18))\n",
    "        \n",
    "        for i, example in enumerate(examples[:num_examples]):\n",
    "            if i >= rows * cols:\n",
    "                break\n",
    "                \n",
    "            # Create subplot with more space for title\n",
    "            ax = fig.add_subplot(rows, cols, i + 1)\n",
    "            \n",
    "            # Add title with input, target and prediction\n",
    "            title = f\"Input: {example['input']}\\nTarget: {example['target']}\\nPred: {example['prediction']}\"\n",
    "            ax.set_title(title, fontsize=12, pad=10)\n",
    "            \n",
    "            # Generate attention heatmap\n",
    "            attention_data = example['attention']\n",
    "            \n",
    "            # Create heatmap with improved appearance\n",
    "            sns.heatmap(\n",
    "                attention_data, \n",
    "                ax=ax,\n",
    "                xticklabels=list(example['input']),\n",
    "                yticklabels=['' for _ in range(len(example['prediction']))],  # Empty y-labels like in your example\n",
    "                cmap='viridis',  # Similar to your example\n",
    "                vmin=0.0,        # Minimum value\n",
    "                vmax=1.0,        # Maximum value\n",
    "                square=True,     # Square cells\n",
    "                cbar_kws={'shrink': 0.8}  # Colorbar settings\n",
    "            )\n",
    "            \n",
    "            # Enhance the appearance of the axis labels\n",
    "            plt.setp(ax.get_xticklabels(), fontsize=12, rotation=0)\n",
    "            plt.setp(ax.get_yticklabels(), fontsize=12, rotation=0)\n",
    "            \n",
    "            # Add colorbar ticks\n",
    "            colorbar = ax.collections[0].colorbar\n",
    "            colorbar.set_ticks([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])\n",
    "            colorbar.set_ticklabels(['0.0', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8'])\n",
    "                \n",
    "        plt.tight_layout(pad=3.0)  # Add padding between subplots\n",
    "        plt.savefig(\"attention_heatmaps.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Generated attention heatmaps for {num_examples} examples\")\n",
    "        return examples\n",
    "    else:\n",
    "        print(\"No examples found\")\n",
    "        return []\n",
    "\n",
    "# ---------------- Run ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        \"embed_size\": 256,\n",
    "        \"hidden_size\": 128,\n",
    "        \"attn_size\": 64,\n",
    "        \"num_layers\": 3,\n",
    "        \"cell_type\": \"GRU\",\n",
    "        \"dropout\": 0.3,\n",
    "        \"batch_size\": 128,\n",
    "        \"lr\": 0.0005722,\n",
    "        \"epochs\": 10,\n",
    "    }\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_pairs = load_pairs(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n",
    "    test_pairs = load_pairs(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\")\n",
    "    input_vocab, output_vocab = build_vocab(train_pairs)\n",
    "    train_dataset = TransliterationDataset(train_pairs, input_vocab, output_vocab)\n",
    "    test_dataset = TransliterationDataset(test_pairs, input_vocab, output_vocab)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    encoder = Encoder(len(input_vocab), config[\"embed_size\"], config[\"hidden_size\"],\n",
    "                      config[\"num_layers\"], config[\"cell_type\"], config[\"dropout\"])\n",
    "    decoder = Decoder(len(output_vocab), config[\"embed_size\"], config[\"hidden_size\"],\n",
    "                      config[\"num_layers\"], config[\"cell_type\"], config[\"dropout\"], config[\"attn_size\"])\n",
    "    model = Seq2Seq(encoder, decoder, output_vocab).to(device)\n",
    "\n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    \n",
    "    # Generate attention heatmaps\n",
    "    print(\"Generating attention heatmaps...\")\n",
    "    examples = generate_attention_heatmap(model, test_loader, input_vocab, output_vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7434963,
     "sourceId": 11834448,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
